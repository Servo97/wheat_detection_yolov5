{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "## CODE USED FROM YOLOv5 REPO https://github.com/ultralytics/yolov5 ##\n",
    "\n",
    "# UTILS #\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "from copy import copy\n",
    "from pathlib import Path\n",
    "from sys import platform\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import yaml\n",
    "from scipy.signal import butter, filtfilt\n",
    "from tqdm import tqdm\n",
    "  # Â torch_utils, google_utils\n",
    "\n",
    "# Set printoptions\n",
    "torch.set_printoptions(linewidth=320, precision=5, profile='long')\n",
    "np.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\n",
    "matplotlib.rc('font', **{'size': 11})\n",
    "\n",
    "# Prevent OpenCV from multithreading (to use PyTorch DataLoader)\n",
    "cv2.setNumThreads(0)\n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "def torch_init_seeds(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Speed-reproducibility tradeoff https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    if seed == 0:  # slower, more reproducible\n",
    "        cudnn.deterministic = True\n",
    "        cudnn.benchmark = False\n",
    "    else:  # faster, less reproducible\n",
    "        cudnn.deterministic = False\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def select_device(device='', apex=False, batch_size=None):\n",
    "    # device = 'cpu' or '0' or '0,1,2,3'\n",
    "    cpu_request = device.lower() == 'cpu'\n",
    "    if device and not cpu_request:  # if device requested other than 'cpu'\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable\n",
    "        assert torch.cuda.is_available(), 'CUDA unavailable, invalid device %s requested' % device  # check availablity\n",
    "\n",
    "    cuda = False if cpu_request else torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        c = 1024 ** 2  # bytes to MB\n",
    "        ng = torch.cuda.device_count()\n",
    "        if ng > 1 and batch_size:  # check that batch_size is compatible with device_count\n",
    "            assert batch_size % ng == 0, 'batch-size %g not multiple of GPU count %g' % (batch_size, ng)\n",
    "        x = [torch.cuda.get_device_properties(i) for i in range(ng)]\n",
    "        s = 'Using CUDA ' + ('Apex ' if apex else '')  # apex for mixed precision https://github.com/NVIDIA/apex\n",
    "        for i in range(0, ng):\n",
    "            if i == 1:\n",
    "                s = ' ' * len(s)\n",
    "            print(\"%sdevice%g _CudaDeviceProperties(name='%s', total_memory=%dMB)\" %\n",
    "                  (s, i, x[i].name, x[i].total_memory / c))\n",
    "    else:\n",
    "        print('Using CPU')\n",
    "\n",
    "    print('')  # skip a line\n",
    "    return torch.device('cuda:0' if cuda else 'cpu')\n",
    "\n",
    "\n",
    "def time_synchronized():\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    return time.time()\n",
    "\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        t = type(m)\n",
    "        if t is nn.Conv2d:\n",
    "            pass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif t is nn.BatchNorm2d:\n",
    "            m.eps = 1e-4\n",
    "            m.momentum = 0.03\n",
    "        elif t in [nn.LeakyReLU, nn.ReLU, nn.ReLU6]:\n",
    "            m.inplace = True\n",
    "\n",
    "\n",
    "def find_modules(model, mclass=nn.Conv2d):\n",
    "    # finds layer indices matching module class 'mclass'\n",
    "    return [i for i, m in enumerate(model.module_list) if isinstance(m, mclass)]\n",
    "\n",
    "\n",
    "def fuse_conv_and_bn(conv, bn):\n",
    "    # https://tehnokv.com/posts/fusing-batchnorm-and-conv/\n",
    "    with torch.no_grad():\n",
    "        # init\n",
    "        fusedconv = torch.nn.Conv2d(conv.in_channels,\n",
    "                                    conv.out_channels,\n",
    "                                    kernel_size=conv.kernel_size,\n",
    "                                    stride=conv.stride,\n",
    "                                    padding=conv.padding,\n",
    "                                    bias=True)\n",
    "\n",
    "        # prepare filters\n",
    "        w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "        w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\n",
    "        fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.size()))\n",
    "\n",
    "        # prepare spatial bias\n",
    "        if conv.bias is not None:\n",
    "            b_conv = conv.bias\n",
    "        else:\n",
    "            b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device)\n",
    "        b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
    "        fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)\n",
    "\n",
    "        return fusedconv\n",
    "\n",
    "\n",
    "def model_info(model, verbose=False):\n",
    "    # Plots a line-by-line description of a PyTorch model\n",
    "    n_p = sum(x.numel() for x in model.parameters())  # number parameters\n",
    "    n_g = sum(x.numel() for x in model.parameters() if x.requires_grad)  # number gradients\n",
    "    if verbose:\n",
    "        print('%5s %40s %9s %12s %20s %10s %10s' % ('layer', 'name', 'gradient', 'parameters', 'shape', 'mu', 'sigma'))\n",
    "        for i, (name, p) in enumerate(model.named_parameters()):\n",
    "            name = name.replace('module_list.', '')\n",
    "            print('%5g %40s %9s %12g %20s %10.3g %10.3g' %\n",
    "                  (i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))\n",
    "\n",
    "    try:  # FLOPS\n",
    "        from thop import profile\n",
    "        macs, _ = profile(model, inputs=(torch.zeros(1, 3, 480, 640),), verbose=False)\n",
    "        fs = ', %.1f GFLOPS' % (macs / 1E9 * 2)\n",
    "    except:\n",
    "        fs = ''\n",
    "\n",
    "    print('Model Summary: %g layers, %g parameters, %g gradients%s' % (len(list(model.parameters())), n_p, n_g, fs))\n",
    "\n",
    "\n",
    "def load_classifier(name='resnet101', n=2):\n",
    "    # Loads a pretrained model reshaped to n-class output\n",
    "    model = models.__dict__[name](pretrained=True)\n",
    "\n",
    "    # Display model properties\n",
    "    input_size = [3, 224, 224]\n",
    "    input_space = 'RGB'\n",
    "    input_range = [0, 1]\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    for x in [input_size, input_space, input_range, mean, std]:\n",
    "        print(x + ' =', eval(x))\n",
    "\n",
    "    # Reshape output to n classes\n",
    "    filters = model.fc.weight.shape[1]\n",
    "    model.fc.bias = torch.nn.Parameter(torch.zeros(n), requires_grad=True)\n",
    "    model.fc.weight = torch.nn.Parameter(torch.zeros(n, filters), requires_grad=True)\n",
    "    model.fc.out_features = n\n",
    "    return model\n",
    "\n",
    "\n",
    "def scale_img(img, ratio=1.0, same_shape=False):  # img(16,3,256,416), r=ratio\n",
    "    # scales img(bs,3,y,x) by ratio\n",
    "    h, w = img.shape[2:]\n",
    "    s = (int(h * ratio), int(w * ratio))  # new size\n",
    "    img = F.interpolate(img, size=s, mode='bilinear', align_corners=False)  # resize\n",
    "    if not same_shape:  # pad/crop img\n",
    "        gs = 32  # (pixels) grid size\n",
    "        h, w = [math.ceil(x * ratio / gs) * gs for x in (h, w)]\n",
    "    return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # value = imagenet mean\n",
    "\n",
    "\n",
    "class ModelEMA:\n",
    "    \"\"\" Model Exponential Moving Average from https://github.com/rwightman/pytorch-image-models\n",
    "    Keep a moving average of everything in the model state_dict (parameters and buffers).\n",
    "    This is intended to allow functionality like\n",
    "    https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
    "    A smoothed version of the weights is necessary for some training schemes to perform well.\n",
    "    E.g. Google's hyper-params for training MNASNet, MobileNet-V3, EfficientNet, etc that use\n",
    "    RMSprop with a short 2.4-3 epoch decay period and slow LR decay rate of .96-.99 requires EMA\n",
    "    smoothing of weights to match results. Pay attention to the decay constant you are using\n",
    "    relative to your update count per epoch.\n",
    "    To keep EMA from using GPU resources, set device='cpu'. This will save a bit of memory but\n",
    "    disable validation of the EMA weights. Validation will have to be done manually in a separate\n",
    "    process, or after the training stops converging.\n",
    "    This class is sensitive where it is initialized in the sequence of model init,\n",
    "    GPU assignment and distributed training wrappers.\n",
    "    I've tested with the sequence in my own train.py for torch.DataParallel, apex.DDP, and single-GPU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay=0.9999, device=''):\n",
    "        # make a copy of the model for accumulating moving average of weights\n",
    "        self.ema = deepcopy(model)\n",
    "        self.ema.eval()\n",
    "        self.updates = 0  # number of EMA updates\n",
    "        self.decay = lambda x: decay * (1 - math.exp(-x / 2000))  # decay exponential ramp (to help early epochs)\n",
    "        self.device = device  # perform ema on different device from model if set\n",
    "        if device:\n",
    "            self.ema.to(device=device)\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        self.updates += 1\n",
    "        d = self.decay(self.updates)\n",
    "        with torch.no_grad():\n",
    "            if type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel):\n",
    "                msd, esd = model.module.state_dict(), self.ema.module.state_dict()\n",
    "            else:\n",
    "                msd, esd = model.state_dict(), self.ema.state_dict()\n",
    "\n",
    "            for k, v in esd.items():\n",
    "                if v.dtype.is_floating_point:\n",
    "                    v *= d\n",
    "                    v += (1. - d) * msd[k].detach()\n",
    "\n",
    "    def update_attr(self, model):\n",
    "        # Assign attributes (which may change during training)\n",
    "        for k in model.__dict__.keys():\n",
    "            if not k.startswith('_'):\n",
    "                setattr(self.ema, k, getattr(model, k))\n",
    "\n",
    "\n",
    "def init_seeds(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch_init_seeds(seed=seed)\n",
    "\n",
    "\n",
    "def check_git_status():\n",
    "    # Suggest 'git pull' if repo is out of date\n",
    "    if platform in ['linux', 'darwin']:\n",
    "        s = subprocess.check_output('if [ -d .git ]; then git fetch && git status -uno; fi', shell=True).decode('utf-8')\n",
    "        if 'Your branch is behind' in s:\n",
    "            print(s[s.find('Your branch is behind'):s.find('\\n\\n')] + '\\n')\n",
    "\n",
    "\n",
    "def check_img_size(img_size, s=32):\n",
    "    # Verify img_size is a multiple of stride s\n",
    "    new_size = make_divisible(img_size, s)  # ceil gs-multiple\n",
    "    if new_size != img_size:\n",
    "        print('WARNING: --img-size %g must be multiple of max stride %g, updating to %g' % (img_size, s, new_size))\n",
    "    return new_size\n",
    "\n",
    "\n",
    "def check_anchors(dataset, model, thr=4.0, imgsz=640):\n",
    "    # Check anchor fit to data, recompute if necessary\n",
    "    print('\\nAnalyzing anchors... ', end='')\n",
    "    m = model.module.model[-1] if hasattr(model, 'module') else model.model[-1]  # Detect()\n",
    "    shapes = imgsz * dataset.shapes / dataset.shapes.max(1, keepdims=True)\n",
    "    scale = np.random.uniform(0.9, 1.1, size=(shapes.shape[0], 1))  # augment scale\n",
    "    wh = torch.tensor(np.concatenate([l[:, 3:5] * s for s, l in zip(shapes * scale, dataset.labels)])).float()  # wh\n",
    "\n",
    "    def metric(k):  # compute metric\n",
    "        r = wh[:, None] / k[None]\n",
    "        x = torch.min(r, 1. / r).min(2)[0]  # ratio metric\n",
    "        best = x.max(1)[0]  # best_x\n",
    "        return (best > 1. / thr).float().mean()  # Â best possible recall\n",
    "\n",
    "    bpr = metric(m.anchor_grid.clone().cpu().view(-1, 2))\n",
    "    print('Best Possible Recall (BPR) = %.4f' % bpr, end='')\n",
    "    if bpr < 0.99:  # threshold to recompute\n",
    "        print('. Attempting to generate improved anchors, please wait...' % bpr)\n",
    "        na = m.anchor_grid.numel() // 2  # number of anchors\n",
    "        new_anchors = kmean_anchors(dataset, n=na, img_size=imgsz, thr=thr, gen=1000, verbose=False)\n",
    "        new_bpr = metric(new_anchors.reshape(-1, 2))\n",
    "        if new_bpr > bpr:  # replace anchors\n",
    "            new_anchors = torch.tensor(new_anchors, device=m.anchors.device).type_as(m.anchors)\n",
    "            m.anchor_grid[:] = new_anchors.clone().view_as(m.anchor_grid)  # for inference\n",
    "            m.anchors[:] = new_anchors.clone().view_as(m.anchors) / m.stride.to(m.anchors.device).view(-1, 1, 1)  # loss\n",
    "            check_anchor_order(m)\n",
    "            print('New anchors saved to model. Update model *.yaml to use these anchors in the future.')\n",
    "        else:\n",
    "            print('Original anchors better than new anchors. Proceeding with original anchors.')\n",
    "    print('')  # newline\n",
    "\n",
    "\n",
    "def check_anchor_order(m):\n",
    "    # Check anchor order against stride order for YOLOv5 Detect() module m, and correct if necessary\n",
    "    a = m.anchor_grid.prod(-1).view(-1)  # anchor area\n",
    "    da = a[-1] - a[0]  # delta a\n",
    "    ds = m.stride[-1] - m.stride[0]  # delta s\n",
    "    if da.sign() != ds.sign():  # same order\n",
    "        m.anchors[:] = m.anchors.flip(0)\n",
    "        m.anchor_grid[:] = m.anchor_grid.flip(0)\n",
    "\n",
    "\n",
    "def check_file(file):\n",
    "    # Searches for file if not found locally\n",
    "    if os.path.isfile(file):\n",
    "        return file\n",
    "    else:\n",
    "        files = glob.glob('./**/' + file, recursive=True)  # find file\n",
    "        assert len(files), 'File Not Found: %s' % file  # assert file was found\n",
    "        return files[0]  # return first file if multiple found\n",
    "\n",
    "\n",
    "def make_divisible(x, divisor):\n",
    "    # Returns x evenly divisble by divisor\n",
    "    return math.ceil(x / divisor) * divisor\n",
    "\n",
    "\n",
    "def labels_to_class_weights(labels, nc=80):\n",
    "    # Get class weights (inverse frequency) from training labels\n",
    "    if labels[0] is None:  # no labels loaded\n",
    "        return torch.Tensor()\n",
    "\n",
    "    labels = np.concatenate(labels, 0)  # labels.shape = (866643, 5) for COCO\n",
    "    classes = labels[:, 0].astype(np.int)  # labels = [class xywh]\n",
    "    weights = np.bincount(classes, minlength=nc)  # occurences per class\n",
    "\n",
    "    # Prepend gridpoint count (for uCE trianing)\n",
    "    # gpi = ((320 / 32 * np.array([1, 2, 4])) ** 2 * 3).sum()  # gridpoints per image\n",
    "    # weights = np.hstack([gpi * len(labels)  - weights.sum() * 9, weights * 9]) ** 0.5  # prepend gridpoints to start\n",
    "\n",
    "    weights[weights == 0] = 1  # replace empty bins with 1\n",
    "    weights = 1 / weights  # number of targets per class\n",
    "    weights /= weights.sum()  # normalize\n",
    "    return torch.from_numpy(weights)\n",
    "\n",
    "\n",
    "def labels_to_image_weights(labels, nc=80, class_weights=np.ones(80)):\n",
    "    # Produces image weights based on class mAPs\n",
    "    n = len(labels)\n",
    "    class_counts = np.array([np.bincount(labels[i][:, 0].astype(np.int), minlength=nc) for i in range(n)])\n",
    "    image_weights = (class_weights.reshape(1, nc) * class_counts).sum(1)\n",
    "    # index = random.choices(range(n), weights=image_weights, k=1)  # weight image sample\n",
    "    return image_weights\n",
    "\n",
    "\n",
    "def coco80_to_coco91_class():  # converts 80-index (val2014) to 91-index (paper)\n",
    "    # https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
    "    # a = np.loadtxt('data/coco.names', dtype='str', delimiter='\\n')\n",
    "    # b = np.loadtxt('data/coco_paper.names', dtype='str', delimiter='\\n')\n",
    "    # x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco\n",
    "    # x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco to darknet\n",
    "    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,\n",
    "         35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n",
    "         64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n",
    "    return x\n",
    "\n",
    "\n",
    "def xyxy2xywh(x):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n",
    "    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n",
    "    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n",
    "    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n",
    "    y[:, 2] = x[:, 2] - x[:, 0]  # width\n",
    "    y[:, 3] = x[:, 3] - x[:, 1]  # height\n",
    "    return y\n",
    "\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "\n",
    "def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):\n",
    "    # Rescale coords (xyxy) from img1_shape to img0_shape\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = max(img1_shape) / max(img0_shape)  # gain  = old / new\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    coords[:, [0, 2]] -= pad[0]  # x padding\n",
    "    coords[:, [1, 3]] -= pad[1]  # y padding\n",
    "    coords[:, :4] /= gain\n",
    "    clip_coords(coords, img0_shape)\n",
    "    return coords\n",
    "\n",
    "\n",
    "def clip_coords(boxes, img_shape):\n",
    "    # Clip bounding xyxy bounding boxes to image shape (height, width)\n",
    "    boxes[:, 0].clamp_(0, img_shape[1])  # x1\n",
    "    boxes[:, 1].clamp_(0, img_shape[0])  # y1\n",
    "    boxes[:, 2].clamp_(0, img_shape[1])  # x2\n",
    "    boxes[:, 3].clamp_(0, img_shape[0])  # y2\n",
    "\n",
    "\n",
    "def ap_per_class(tp, conf, pred_cls, target_cls):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "    # Arguments\n",
    "        tp:    True positives (nparray, nx1 or nx10).\n",
    "        conf:  Objectness value from 0-1 (nparray).\n",
    "        pred_cls: Predicted object classes (nparray).\n",
    "        target_cls: True object classes (nparray).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by objectness\n",
    "    i = np.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes = np.unique(target_cls)\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    pr_score = 0.1  # score to evaluate P and R https://github.com/ultralytics/yolov3/issues/898\n",
    "    s = [unique_classes.shape[0], tp.shape[1]]  # number class, number iou thresholds (i.e. 10 for mAP0.5...0.95)\n",
    "    ap, p, r = np.zeros(s), np.zeros(s), np.zeros(s)\n",
    "    for ci, c in enumerate(unique_classes):\n",
    "        i = pred_cls == c\n",
    "        n_gt = (target_cls == c).sum()  # Number of ground truth objects\n",
    "        n_p = i.sum()  # Number of predicted objects\n",
    "\n",
    "        if n_p == 0 or n_gt == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # Accumulate FPs and TPs\n",
    "            fpc = (1 - tp[i]).cumsum(0)\n",
    "            tpc = tp[i].cumsum(0)\n",
    "\n",
    "            # Recall\n",
    "            recall = tpc / (n_gt + 1e-16)  # recall curve\n",
    "            r[ci] = np.interp(-pr_score, -conf[i], recall[:, 0])  # r at pr_score, negative x, xp because xp decreases\n",
    "\n",
    "            # Precision\n",
    "            precision = tpc / (tpc + fpc)  # precision curve\n",
    "            p[ci] = np.interp(-pr_score, -conf[i], precision[:, 0])  # p at pr_score\n",
    "\n",
    "            # AP from recall-precision curve\n",
    "            for j in range(tp.shape[1]):\n",
    "                ap[ci, j] = compute_ap(recall[:, j], precision[:, j])\n",
    "\n",
    "            # Plot\n",
    "            # fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "            # ax.plot(recall, precision)\n",
    "            # ax.set_xlabel('Recall')\n",
    "            # ax.set_ylabel('Precision')\n",
    "            # ax.set_xlim(0, 1.01)\n",
    "            # ax.set_ylim(0, 1.01)\n",
    "            # fig.tight_layout()\n",
    "            # fig.savefig('PR_curve.png', dpi=300)\n",
    "\n",
    "    # Compute F1 score (harmonic mean of precision and recall)\n",
    "    f1 = 2 * p * r / (p + r + 1e-16)\n",
    "\n",
    "    return p, r, ap, f1, unique_classes.astype('int32')\n",
    "\n",
    "\n",
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "\n",
    "    # Append sentinel values to beginning and end\n",
    "    mrec = np.concatenate(([0.], recall, [min(recall[-1] + 1E-3, 1.)]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # Compute the precision envelope\n",
    "    mpre = np.flip(np.maximum.accumulate(np.flip(mpre)))\n",
    "\n",
    "    # Integrate area under curve\n",
    "    method = 'interp'  # methods: 'continuous', 'interp'\n",
    "    if method == 'interp':\n",
    "        x = np.linspace(0, 1, 101)  # 101-point interp (COCO)\n",
    "        ap = np.trapz(np.interp(x, mrec, mpre), x)  # integrate\n",
    "    else:  # 'continuous'\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]  # points where x axis (recall) changes\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])  # area under curve\n",
    "\n",
    "    return ap\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False):\n",
    "    # Returns the IoU of box1 to box2. box1 is 4, box2 is nx4\n",
    "    box2 = box2.t()\n",
    "\n",
    "    # Get the coordinates of bounding boxes\n",
    "    if x1y1x2y2:  # x1, y1, x2, y2 = box1\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n",
    "    else:  # transform from xywh to xyxy\n",
    "        b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2\n",
    "        b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2\n",
    "        b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2\n",
    "        b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2\n",
    "\n",
    "    # Intersection area\n",
    "    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
    "            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
    "\n",
    "    # Union Area\n",
    "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1\n",
    "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1\n",
    "    union = (w1 * h1 + 1e-16) + w2 * h2 - inter\n",
    "\n",
    "    iou = inter / union  # iou\n",
    "    if GIoU or DIoU or CIoU:\n",
    "        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)  # convex (smallest enclosing box) width\n",
    "        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height\n",
    "        if GIoU:  # Generalized IoU https://arxiv.org/pdf/1902.09630.pdf\n",
    "            c_area = cw * ch + 1e-16  # convex area\n",
    "            return iou - (c_area - union) / c_area  # GIoU\n",
    "        if DIoU or CIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n",
    "            # convex diagonal squared\n",
    "            c2 = cw ** 2 + ch ** 2 + 1e-16\n",
    "            # centerpoint distance squared\n",
    "            rho2 = ((b2_x1 + b2_x2) - (b1_x1 + b1_x2)) ** 2 / 4 + ((b2_y1 + b2_y2) - (b1_y1 + b1_y2)) ** 2 / 4\n",
    "            if DIoU:\n",
    "                return iou - rho2 / c2  # DIoU\n",
    "            elif CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
    "                v = (4 / math.pi ** 2) * torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)\n",
    "                with torch.no_grad():\n",
    "                    alpha = v / (1 - iou + v)\n",
    "                return iou - (rho2 / c2 + v * alpha)  # CIoU\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def box_iou(box1, box2):\n",
    "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    def box_area(box):\n",
    "        # box = 4xn\n",
    "        return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "    area1 = box_area(box1.t())\n",
    "    area2 = box_area(box2.t())\n",
    "\n",
    "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n",
    "    return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)\n",
    "\n",
    "\n",
    "def wh_iou(wh1, wh2):\n",
    "    # Returns the nxm IoU matrix. wh1 is nx2, wh2 is mx2\n",
    "    wh1 = wh1[:, None]  # [N,1,2]\n",
    "    wh2 = wh2[None]  # [1,M,2]\n",
    "    inter = torch.min(wh1, wh2).prod(2)  # [N,M]\n",
    "    return inter / (wh1.prod(2) + wh2.prod(2) - inter)  # iou = inter / (area1 + area2 - inter)\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    # Wraps focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5)\n",
    "    def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.loss_fcn = loss_fcn  # must be nn.BCEWithLogitsLoss()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = loss_fcn.reduction\n",
    "        self.loss_fcn.reduction = 'none'  # required to apply FL to each element\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        loss = self.loss_fcn(pred, true)\n",
    "        # p_t = torch.exp(-loss)\n",
    "        # loss *= self.alpha * (1.000001 - p_t) ** self.gamma  # non-zero power for gradient stability\n",
    "\n",
    "        # TF implementation https://github.com/tensorflow/addons/blob/v0.7.1/tensorflow_addons/losses/focal_loss.py\n",
    "        pred_prob = torch.sigmoid(pred)  # prob from logits\n",
    "        p_t = true * pred_prob + (1 - true) * (1 - pred_prob)\n",
    "        alpha_factor = true * self.alpha + (1 - true) * (1 - self.alpha)\n",
    "        modulating_factor = (1.0 - p_t) ** self.gamma\n",
    "        loss *= alpha_factor * modulating_factor\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:  # 'none'\n",
    "            return loss\n",
    "\n",
    "\n",
    "def smooth_BCE(eps=0.1):  # https://github.com/ultralytics/yolov3/issues/238#issuecomment-598028441\n",
    "    # return positive, negative label smoothing BCE targets\n",
    "    return 1.0 - 0.5 * eps, 0.5 * eps\n",
    "\n",
    "\n",
    "class BCEBlurWithLogitsLoss(nn.Module):\n",
    "    # BCEwithLogitLoss() with reduced missing label effects.\n",
    "    def __init__(self, alpha=0.05):\n",
    "        super(BCEBlurWithLogitsLoss, self).__init__()\n",
    "        self.loss_fcn = nn.BCEWithLogitsLoss(reduction='none')  # must be nn.BCEWithLogitsLoss()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        loss = self.loss_fcn(pred, true)\n",
    "        pred = torch.sigmoid(pred)  # prob from logits\n",
    "        dx = pred - true  # reduce only missing label effects\n",
    "        # dx = (pred - true).abs()  # reduce missing label and false label effects\n",
    "        alpha_factor = 1 - torch.exp((dx - 1) / (self.alpha + 1e-4))\n",
    "        loss *= alpha_factor\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def compute_loss(p, targets, model):  # predictions, targets, model\n",
    "    ft = torch.cuda.FloatTensor if p[0].is_cuda else torch.Tensor\n",
    "    lcls, lbox, lobj = ft([0]), ft([0]), ft([0])\n",
    "    tcls, tbox, indices, anchors = build_targets(p, targets, model)  # targets\n",
    "    h = model.hyp  # hyperparameters\n",
    "    red = 'mean'  # Loss reduction (sum or mean)\n",
    "\n",
    "    # Define criteria\n",
    "    BCEcls = nn.BCEWithLogitsLoss(pos_weight=ft([h['cls_pw']]), reduction=red)\n",
    "    BCEobj = nn.BCEWithLogitsLoss(pos_weight=ft([h['obj_pw']]), reduction=red)\n",
    "\n",
    "    # class label smoothing https://arxiv.org/pdf/1902.04103.pdf eqn 3\n",
    "    cp, cn = smooth_BCE(eps=0.0)\n",
    "\n",
    "    # focal loss\n",
    "    g = h['fl_gamma']  # focal loss gamma\n",
    "    if g > 0:\n",
    "        BCEcls, BCEobj = FocalLoss(BCEcls, g), FocalLoss(BCEobj, g)\n",
    "\n",
    "    # per output\n",
    "    nt = 0  # targets\n",
    "    for i, pi in enumerate(p):  # layer index, layer predictions\n",
    "        b, a, gj, gi = indices[i]  # image, anchor, gridy, gridx\n",
    "        tobj = torch.zeros_like(pi[..., 0])  # target obj\n",
    "\n",
    "        nb = b.shape[0]  # number of targets\n",
    "        if nb:\n",
    "            nt += nb  # cumulative targets\n",
    "            ps = pi[b, a, gj, gi]  # prediction subset corresponding to targets\n",
    "\n",
    "            # GIoU\n",
    "            pxy = ps[:, :2].sigmoid() * 2. - 0.5\n",
    "            pwh = (ps[:, 2:4].sigmoid() * 2) ** 2 * anchors[i]\n",
    "            pbox = torch.cat((pxy, pwh), 1)  # predicted box\n",
    "            giou = bbox_iou(pbox.t(), tbox[i], x1y1x2y2=False, GIoU=True)  # giou(prediction, target)\n",
    "            lbox += (1.0 - giou).sum() if red == 'sum' else (1.0 - giou).mean()  # giou loss\n",
    "\n",
    "            # Obj\n",
    "            tobj[b, a, gj, gi] = (1.0 - model.gr) + model.gr * giou.detach().clamp(0).type(tobj.dtype)  # giou ratio\n",
    "\n",
    "            # Class\n",
    "            if model.nc > 1:  # cls loss (only if multiple classes)\n",
    "                t = torch.full_like(ps[:, 5:], cn)  # targets\n",
    "                t[range(nb), tcls[i]] = cp\n",
    "                lcls += BCEcls(ps[:, 5:], t)  # BCE\n",
    "\n",
    "            # Append targets to text file\n",
    "            # with open('targets.txt', 'a') as file:\n",
    "            #     [file.write('%11.5g ' * 4 % tuple(x) + '\\n') for x in torch.cat((txy[i], twh[i]), 1)]\n",
    "\n",
    "        lobj += BCEobj(pi[..., 4], tobj)  # obj loss\n",
    "\n",
    "    lbox *= h['giou']\n",
    "    lobj *= h['obj']\n",
    "    lcls *= h['cls']\n",
    "    bs = tobj.shape[0]  # batch size\n",
    "    if red == 'sum':\n",
    "        g = 3.0  # loss gain\n",
    "        lobj *= g / bs\n",
    "        if nt:\n",
    "            lcls *= g / nt / model.nc\n",
    "            lbox *= g / nt\n",
    "\n",
    "    loss = lbox + lobj + lcls\n",
    "    return loss * bs, torch.cat((lbox, lobj, lcls, loss)).detach()\n",
    "\n",
    "\n",
    "def build_targets(p, targets, model):\n",
    "    # Build targets for compute_loss(), input targets(image,class,x,y,w,h)\n",
    "    det = model.module.model[-1] if type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel) \\\n",
    "        else model.model[-1]  # Detect() module\n",
    "    na, nt = det.na, targets.shape[0]  # number of anchors, targets\n",
    "    tcls, tbox, indices, anch = [], [], [], []\n",
    "    gain = torch.ones(6, device=targets.device)  # normalized to gridspace gain\n",
    "    off = torch.tensor([[1, 0], [0, 1], [-1, 0], [0, -1]], device=targets.device).float()  # overlap offsets\n",
    "    at = torch.arange(na).view(na, 1).repeat(1, nt)  # anchor tensor, same as .repeat_interleave(nt)\n",
    "\n",
    "    style = 'rect4'\n",
    "    for i in range(det.nl):\n",
    "        anchors = det.anchors[i]\n",
    "        gain[2:] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]  # xyxy gain\n",
    "\n",
    "        # Match targets to anchors\n",
    "        a, t, offsets = [], targets * gain, 0\n",
    "        if nt:\n",
    "            r = t[None, :, 4:6] / anchors[:, None]  # wh ratio\n",
    "            j = torch.max(r, 1. / r).max(2)[0] < model.hyp['anchor_t']  # compare\n",
    "            # j = wh_iou(anchors, t[:, 4:6]) > model.hyp['iou_t']  # iou(3,n) = wh_iou(anchors(3,2), gwh(n,2))\n",
    "            a, t = at[j], t.repeat(na, 1, 1)[j]  # filter\n",
    "\n",
    "            # overlaps\n",
    "            gxy = t[:, 2:4]  # grid xy\n",
    "            z = torch.zeros_like(gxy)\n",
    "            if style == 'rect2':\n",
    "                g = 0.2  # offset\n",
    "                j, k = ((gxy % 1. < g) & (gxy > 1.)).T\n",
    "                a, t = torch.cat((a, a[j], a[k]), 0), torch.cat((t, t[j], t[k]), 0)\n",
    "                offsets = torch.cat((z, z[j] + off[0], z[k] + off[1]), 0) * g\n",
    "\n",
    "            elif style == 'rect4':\n",
    "                g = 0.5  # offset\n",
    "                j, k = ((gxy % 1. < g) & (gxy > 1.)).T\n",
    "                l, m = ((gxy % 1. > (1 - g)) & (gxy < (gain[[2, 3]] - 1.))).T\n",
    "                a, t = torch.cat((a, a[j], a[k], a[l], a[m]), 0), torch.cat((t, t[j], t[k], t[l], t[m]), 0)\n",
    "                offsets = torch.cat((z, z[j] + off[0], z[k] + off[1], z[l] + off[2], z[m] + off[3]), 0) * g\n",
    "\n",
    "        # Define\n",
    "        b, c = t[:, :2].long().T  # image, class\n",
    "        gxy = t[:, 2:4]  # grid xy\n",
    "        gwh = t[:, 4:6]  # grid wh\n",
    "        gij = (gxy - offsets).long()\n",
    "        gi, gj = gij.T  # grid xy indices\n",
    "\n",
    "        # Append\n",
    "        indices.append((b, a, gj, gi))  # image, anchor, grid indices\n",
    "        tbox.append(torch.cat((gxy - gij, gwh), 1))  # box\n",
    "        anch.append(anchors[a])  # anchors\n",
    "        tcls.append(c)  # class\n",
    "\n",
    "    return tcls, tbox, indices, anch\n",
    "\n",
    "\n",
    "def non_max_suppression(prediction, conf_thres=0.1, iou_thres=0.6, merge=False, classes=None, agnostic=False):\n",
    "    \"\"\"Performs Non-Maximum Suppression (NMS) on inference results\n",
    "\n",
    "    Returns:\n",
    "         detections with shape: nx6 (x1, y1, x2, y2, conf, cls)\n",
    "    \"\"\"\n",
    "    if prediction.dtype is torch.float16:\n",
    "        prediction = prediction.float()  # to FP32\n",
    "\n",
    "    nc = prediction[0].shape[1] - 5  # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "    # Settings\n",
    "    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
    "    max_det = 300  # maximum number of detections per image\n",
    "    time_limit = 10.0  # seconds to quit after\n",
    "    redundant = True  # require redundant detections\n",
    "    multi_label = nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "\n",
    "    t = time.time()\n",
    "    output = [None] * prediction.shape[0]\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[xc[xi]]  # confidence\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "        box = xywh2xyxy(x[:, :4])\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        if multi_label:\n",
    "            i, j = (x[:, 5:] > conf_thres).nonzero().t()\n",
    "            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = x[:, 5:].max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Filter by class\n",
    "        if classes:\n",
    "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "\n",
    "        # Apply finite constraint\n",
    "        # if not torch.isfinite(x).all():\n",
    "        #     x = x[torch.isfinite(x).all(1)]\n",
    "\n",
    "        # If none remain process next image\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:\n",
    "            continue\n",
    "\n",
    "        # Sort by confidence\n",
    "        # x = x[x[:, 4].argsort(descending=True)]\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.boxes.nms(boxes, scores, iou_thres)\n",
    "        if i.shape[0] > max_det:  # limit detections\n",
    "            i = i[:max_det]\n",
    "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "            try:  # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "                weights = iou * scores[None]  # box weights\n",
    "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "                if redundant:\n",
    "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
    "            except:  # possible CUDA error https://github.com/ultralytics/yolov3/issues/1139\n",
    "                print(x, i, x.shape, i.shape)\n",
    "                pass\n",
    "\n",
    "        output[xi] = x[i]\n",
    "        if (time.time() - t) > time_limit:\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def strip_optimizer(f='weights/best.pt'):  # from utils.utils import *; strip_optimizer()\n",
    "    # Strip optimizer from *.pt files for lighter files (reduced by 1/2 size)\n",
    "    x = torch.load(f, map_location=torch.device('cpu'))\n",
    "    x['optimizer'] = None\n",
    "    x['model'].half()  # to FP16\n",
    "    torch.save(x, f)\n",
    "    print('Optimizer stripped from %s' % f)\n",
    "\n",
    "\n",
    "def create_pretrained(f='weights/best.pt', s='weights/pretrained.pt'):  # from utils.utils import *; create_pretrained()\n",
    "    # create pretrained checkpoint 's' from 'f' (create_pretrained(x, x) for x in glob.glob('./*.pt'))\n",
    "    device = torch.device('cpu')\n",
    "    x = torch.load(s, map_location=device)\n",
    "\n",
    "    x['optimizer'] = None\n",
    "    x['training_results'] = None\n",
    "    x['epoch'] = -1\n",
    "    x['model'].half()  # to FP16\n",
    "    for p in x['model'].parameters():\n",
    "        p.requires_grad = True\n",
    "    torch.save(x, s)\n",
    "    print('%s saved as pretrained checkpoint %s' % (f, s))\n",
    "\n",
    "\n",
    "def coco_class_count(path='../coco/labels/train2014/'):\n",
    "    # Histogram of occurrences per class\n",
    "    nc = 80  # number classes\n",
    "    x = np.zeros(nc, dtype='int32')\n",
    "    files = sorted(glob.glob('%s/*.*' % path))\n",
    "    for i, file in enumerate(files):\n",
    "        labels = np.loadtxt(file, dtype=np.float32).reshape(-1, 5)\n",
    "        x += np.bincount(labels[:, 0].astype('int32'), minlength=nc)\n",
    "        print(i, len(files))\n",
    "\n",
    "\n",
    "def coco_only_people(path='../coco/labels/train2017/'):  # from utils.utils import *; coco_only_people()\n",
    "    # Find images with only people\n",
    "    files = sorted(glob.glob('%s/*.*' % path))\n",
    "    for i, file in enumerate(files):\n",
    "        labels = np.loadtxt(file, dtype=np.float32).reshape(-1, 5)\n",
    "        if all(labels[:, 0] == 0):\n",
    "            print(labels.shape[0], file)\n",
    "\n",
    "\n",
    "def crop_images_random(path='../images/', scale=0.50):  # from utils.utils import *; crop_images_random()\n",
    "    # crops images into random squares up to scale fraction\n",
    "    # WARNING: overwrites images!\n",
    "    for file in tqdm(sorted(glob.glob('%s/*.*' % path))):\n",
    "        img = cv2.imread(file)  # BGR\n",
    "        if img is not None:\n",
    "            h, w = img.shape[:2]\n",
    "\n",
    "            # create random mask\n",
    "            a = 30  # minimum size (pixels)\n",
    "            mask_h = random.randint(a, int(max(a, h * scale)))  # mask height\n",
    "            mask_w = mask_h  # mask width\n",
    "\n",
    "            # box\n",
    "            xmin = max(0, random.randint(0, w) - mask_w // 2)\n",
    "            ymin = max(0, random.randint(0, h) - mask_h // 2)\n",
    "            xmax = min(w, xmin + mask_w)\n",
    "            ymax = min(h, ymin + mask_h)\n",
    "\n",
    "            # apply random color mask\n",
    "            cv2.imwrite(file, img[ymin:ymax, xmin:xmax])\n",
    "\n",
    "\n",
    "def coco_single_class_labels(path='../coco/labels/train2014/', label_class=43):\n",
    "    # Makes single-class coco datasets. from utils.utils import *; coco_single_class_labels()\n",
    "    if os.path.exists('new/'):\n",
    "        shutil.rmtree('new/')  # delete output folder\n",
    "    os.makedirs('new/')  # make new output folder\n",
    "    os.makedirs('new/labels/')\n",
    "    os.makedirs('new/images/')\n",
    "    for file in tqdm(sorted(glob.glob('%s/*.*' % path))):\n",
    "        with open(file, 'r') as f:\n",
    "            labels = np.array([x.split() for x in f.read().splitlines()], dtype=np.float32)\n",
    "        i = labels[:, 0] == label_class\n",
    "        if any(i):\n",
    "            img_file = file.replace('labels', 'images').replace('txt', 'jpg')\n",
    "            labels[:, 0] = 0  # reset class to 0\n",
    "            with open('new/images.txt', 'a') as f:  # add image to dataset list\n",
    "                f.write(img_file + '\\n')\n",
    "            with open('new/labels/' + Path(file).name, 'a') as f:  # write label\n",
    "                for l in labels[i]:\n",
    "                    f.write('%g %.6f %.6f %.6f %.6f\\n' % tuple(l))\n",
    "            shutil.copyfile(src=img_file, dst='new/images/' + Path(file).name.replace('txt', 'jpg'))  # copy images\n",
    "\n",
    "\n",
    "def kmean_anchors(path='./data/coco128.yaml', n=9, img_size=640, thr=4.0, gen=1000, verbose=True):\n",
    "    \"\"\" Creates kmeans-evolved anchors from training dataset\n",
    "\n",
    "        Arguments:\n",
    "            path: path to dataset *.yaml, or a loaded dataset\n",
    "            n: number of anchors\n",
    "            img_size: image size used for training\n",
    "            thr: anchor-label wh ratio threshold hyperparameter hyp['anchor_t'] used for training, default=4.0\n",
    "            gen: generations to evolve anchors using genetic algorithm\n",
    "\n",
    "        Return:\n",
    "            k: kmeans evolved anchors\n",
    "\n",
    "        Usage:\n",
    "            from utils.utils import *; _ = kmean_anchors()\n",
    "    \"\"\"\n",
    "    thr = 1. / thr\n",
    "\n",
    "    def metric(k, wh):  # compute metrics\n",
    "        r = wh[:, None] / k[None]\n",
    "        x = torch.min(r, 1. / r).min(2)[0]  # ratio metric\n",
    "        # x = wh_iou(wh, torch.tensor(k))  # iou metric\n",
    "        return x, x.max(1)[0]  # x, best_x\n",
    "\n",
    "    def fitness(k):  # mutation fitness\n",
    "        _, best = metric(torch.tensor(k, dtype=torch.float32), wh)\n",
    "        return (best * (best > thr).float()).mean()  # fitness\n",
    "\n",
    "    def print_results(k):\n",
    "        k = k[np.argsort(k.prod(1))]  # sort small to large\n",
    "        x, best = metric(k, wh0)\n",
    "        bpr, aat = (best > thr).float().mean(), (x > thr).float().mean() * n  # best possible recall, anch > thr\n",
    "        print('thr=%.2f: %.4f best possible recall, %.2f anchors past thr' % (thr, bpr, aat))\n",
    "        print('n=%g, img_size=%s, metric_all=%.3f/%.3f-mean/best, past_thr=%.3f-mean: ' %\n",
    "              (n, img_size, x.mean(), best.mean(), x[x > thr].mean()), end='')\n",
    "        for i, x in enumerate(k):\n",
    "            print('%i,%i' % (round(x[0]), round(x[1])), end=',  ' if i < len(k) - 1 else '\\n')  # use in *.cfg\n",
    "        return k\n",
    "\n",
    "    if isinstance(path, str):  # *.yaml file\n",
    "        with open(path) as f:\n",
    "            data_dict = yaml.load(f, Loader=yaml.FullLoader)  # model dict\n",
    "        from utils.datasets import LoadImagesAndLabels\n",
    "        dataset = LoadImagesAndLabels(data_dict['train'], augment=True, rect=True)\n",
    "    else:\n",
    "        dataset = path  # dataset\n",
    "\n",
    "    # Get label wh\n",
    "    shapes = img_size * dataset.shapes / dataset.shapes.max(1, keepdims=True)\n",
    "    wh0 = np.concatenate([l[:, 3:5] * s for s, l in zip(shapes, dataset.labels)])  # wh\n",
    "\n",
    "    # Filter\n",
    "    i = (wh0 < 4.0).any(1).sum()\n",
    "    if i:\n",
    "        print('WARNING: Extremely small objects found. '\n",
    "              '%g of %g labels are < 4 pixels in width or height.' % (i, len(wh0)))\n",
    "    wh = wh0[(wh0 >= 4.0).any(1)]  # filter > 2 pixels\n",
    "\n",
    "    # Kmeans calculation\n",
    "    from scipy.cluster.vq import kmeans\n",
    "    print('Running kmeans for %g anchors on %g points...' % (n, len(wh)))\n",
    "    s = wh.std(0)  # sigmas for whitening\n",
    "    k, dist = kmeans(wh / s, n, iter=30)  # points, mean distance\n",
    "    k *= s\n",
    "    wh = torch.tensor(wh, dtype=torch.float32)  # filtered\n",
    "    wh0 = torch.tensor(wh0, dtype=torch.float32)  # unflitered\n",
    "    k = print_results(k)\n",
    "\n",
    "    # Plot\n",
    "    # k, d = [None] * 20, [None] * 20\n",
    "    # for i in tqdm(range(1, 21)):\n",
    "    #     k[i-1], d[i-1] = kmeans(wh / s, i)  # points, mean distance\n",
    "    # fig, ax = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    # ax = ax.ravel()\n",
    "    # ax[0].plot(np.arange(1, 21), np.array(d) ** 2, marker='.')\n",
    "    # fig, ax = plt.subplots(1, 2, figsize=(14, 7))  # plot wh\n",
    "    # ax[0].hist(wh[wh[:, 0]<100, 0],400)\n",
    "    # ax[1].hist(wh[wh[:, 1]<100, 1],400)\n",
    "    # fig.tight_layout()\n",
    "    # fig.savefig('wh.png', dpi=200)\n",
    "\n",
    "    # Evolve\n",
    "    npr = np.random\n",
    "    f, sh, mp, s = fitness(k), k.shape, 0.9, 0.1  # fitness, generations, mutation prob, sigma\n",
    "    pbar = tqdm(range(gen), desc='Evolving anchors with Genetic Algorithm')  # progress bar\n",
    "    for _ in pbar:\n",
    "        v = np.ones(sh)\n",
    "        while (v == 1).all():  # mutate until a change occurs (prevent duplicates)\n",
    "            v = ((npr.random(sh) < mp) * npr.random() * npr.randn(*sh) * s + 1).clip(0.3, 3.0)\n",
    "        kg = (k.copy() * v).clip(min=2.0)\n",
    "        fg = fitness(kg)\n",
    "        if fg > f:\n",
    "            f, k = fg, kg.copy()\n",
    "            pbar.desc = 'Evolving anchors with Genetic Algorithm: fitness = %.4f' % f\n",
    "            if verbose:\n",
    "                print_results(k)\n",
    "\n",
    "    return print_results(k)\n",
    "\n",
    "\n",
    "def print_mutation(hyp, results, bucket=''):\n",
    "    # Print mutation results to evolve.txt (for use with train.py --evolve)\n",
    "    a = '%10s' * len(hyp) % tuple(hyp.keys())  # hyperparam keys\n",
    "    b = '%10.3g' * len(hyp) % tuple(hyp.values())  # hyperparam values\n",
    "    c = '%10.4g' * len(results) % results  # results (P, R, mAP, F1, test_loss)\n",
    "    print('\\n%s\\n%s\\nEvolved fitness: %s\\n' % (a, b, c))\n",
    "\n",
    "    if bucket:\n",
    "        os.system('gsutil cp gs://%s/evolve.txt .' % bucket)  # download evolve.txt\n",
    "\n",
    "    with open('evolve.txt', 'a') as f:  # append result\n",
    "        f.write(c + b + '\\n')\n",
    "    x = np.unique(np.loadtxt('evolve.txt', ndmin=2), axis=0)  # load unique rows\n",
    "    np.savetxt('evolve.txt', x[np.argsort(-fitness(x))], '%10.3g')  # save sort by fitness\n",
    "\n",
    "    if bucket:\n",
    "        os.system('gsutil cp evolve.txt gs://%s' % bucket)  # upload evolve.txt\n",
    "\n",
    "\n",
    "def apply_classifier(x, model, img, im0):\n",
    "    # applies a second stage classifier to yolo outputs\n",
    "    im0 = [im0] if isinstance(im0, np.ndarray) else im0\n",
    "    for i, d in enumerate(x):  # per image\n",
    "        if d is not None and len(d):\n",
    "            d = d.clone()\n",
    "\n",
    "            # Reshape and pad cutouts\n",
    "            b = xyxy2xywh(d[:, :4])  # boxes\n",
    "            b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # rectangle to square\n",
    "            b[:, 2:] = b[:, 2:] * 1.3 + 30  # pad\n",
    "            d[:, :4] = xywh2xyxy(b).long()\n",
    "\n",
    "            # Rescale boxes from img_size to im0 size\n",
    "            scale_coords(img.shape[2:], d[:, :4], im0[i].shape)\n",
    "\n",
    "            # Classes\n",
    "            pred_cls1 = d[:, 5].long()\n",
    "            ims = []\n",
    "            for j, a in enumerate(d):  # per item\n",
    "                cutout = im0[i][int(a[1]):int(a[3]), int(a[0]):int(a[2])]\n",
    "                im = cv2.resize(cutout, (224, 224))  # BGR\n",
    "                # cv2.imwrite('test%i.jpg' % j, cutout)\n",
    "\n",
    "                im = im[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "                im = np.ascontiguousarray(im, dtype=np.float32)  # uint8 to float32\n",
    "                im /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "                ims.append(im)\n",
    "\n",
    "            pred_cls2 = model(torch.Tensor(ims).to(d.device)).argmax(1)  # classifier prediction\n",
    "            x[i] = x[i][pred_cls1 == pred_cls2]  # retain matching class detections\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def fitness(x):\n",
    "    # Returns fitness (for use with results.txt or evolve.txt)\n",
    "    w = [0.0, 0.0, 0.1, 0.9]  # weights for [P, R, mAP@0.5, mAP@0.5:0.95]\n",
    "    return (x[:, :4] * w).sum(1)\n",
    "\n",
    "\n",
    "def output_to_target(output, width, height):\n",
    "    \"\"\"\n",
    "    Convert a YOLO model output to target format\n",
    "    [batch_id, class_id, x, y, w, h, conf]\n",
    "    \"\"\"\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        output = output.cpu().numpy()\n",
    "\n",
    "    targets = []\n",
    "    for i, o in enumerate(output):\n",
    "        if o is not None:\n",
    "            for pred in o:\n",
    "                box = pred[:4]\n",
    "                w = (box[2] - box[0]) / width\n",
    "                h = (box[3] - box[1]) / height\n",
    "                x = box[0] / width + w / 2\n",
    "                y = box[1] / height + h / 2\n",
    "                conf = pred[4]\n",
    "                cls = int(pred[5])\n",
    "\n",
    "                targets.append([i, cls, x, y, w, h, conf])\n",
    "\n",
    "    return np.array(targets)\n",
    "\n",
    "\n",
    "# Plotting functions ---------------------------------------------------------------------------------------------------\n",
    "def butter_lowpass_filtfilt(data, cutoff=1500, fs=50000, order=5):\n",
    "    # https://stackoverflow.com/questions/28536191/how-to-filter-smooth-with-scipy-numpy\n",
    "    def butter_lowpass(cutoff, fs, order):\n",
    "        nyq = 0.5 * fs\n",
    "        normal_cutoff = cutoff / nyq\n",
    "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "        return b, a\n",
    "\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    return filtfilt(b, a, data)  # forward-backward filter\n",
    "\n",
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=None):\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "\n",
    "def plot_wh_methods():  # from utils.utils import *; plot_wh_methods()\n",
    "    # Compares the two methods for width-height anchor multiplication\n",
    "    # https://github.com/ultralytics/yolov3/issues/168\n",
    "    x = np.arange(-4.0, 4.0, .1)\n",
    "    ya = np.exp(x)\n",
    "    yb = torch.sigmoid(torch.from_numpy(x)).numpy() * 2\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 3), dpi=150)\n",
    "    plt.plot(x, ya, '.-', label='yolo method')\n",
    "    plt.plot(x, yb ** 2, '.-', label='^2 power method')\n",
    "    plt.plot(x, yb ** 2.5, '.-', label='^2.5 power method')\n",
    "    plt.xlim(left=-4, right=4)\n",
    "    plt.ylim(bottom=0, top=6)\n",
    "    plt.xlabel('input')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('comparison.png', dpi=200)\n",
    "\n",
    "\n",
    "def plot_images(images, targets, paths=None, fname='images.jpg', names=None, max_size=640, max_subplots=16):\n",
    "    tl = 3  # line thickness\n",
    "    tf = max(tl - 1, 1)  # font thickness\n",
    "    if os.path.isfile(fname):  # do not overwrite\n",
    "        return None\n",
    "\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        images = images.cpu().float().numpy()\n",
    "\n",
    "    if isinstance(targets, torch.Tensor):\n",
    "        targets = targets.cpu().numpy()\n",
    "\n",
    "    # un-normalise\n",
    "    if np.max(images[0]) <= 1:\n",
    "        images *= 255\n",
    "\n",
    "    bs, _, h, w = images.shape  # batch size, _, height, width\n",
    "    bs = min(bs, max_subplots)  # limit plot images\n",
    "    ns = np.ceil(bs ** 0.5)  # number of subplots (square)\n",
    "\n",
    "    # Check if we should resize\n",
    "    scale_factor = max_size / max(h, w)\n",
    "    if scale_factor < 1:\n",
    "        h = math.ceil(scale_factor * h)\n",
    "        w = math.ceil(scale_factor * w)\n",
    "\n",
    "    # Empty array for output\n",
    "    mosaic = np.full((int(ns * h), int(ns * w), 3), 255, dtype=np.uint8)\n",
    "\n",
    "    # Fix class - colour map\n",
    "    prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "    # https://stackoverflow.com/questions/51350872/python-from-color-name-to-rgb\n",
    "    hex2rgb = lambda h: tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n",
    "    color_lut = [hex2rgb(h) for h in prop_cycle.by_key()['color']]\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        if i == max_subplots:  # if last batch has fewer images than we expect\n",
    "            break\n",
    "\n",
    "        block_x = int(w * (i // ns))\n",
    "        block_y = int(h * (i % ns))\n",
    "\n",
    "        img = img.transpose(1, 2, 0)\n",
    "        if scale_factor < 1:\n",
    "            img = cv2.resize(img, (w, h))\n",
    "\n",
    "        mosaic[block_y:block_y + h, block_x:block_x + w, :] = img\n",
    "        if len(targets) > 0:\n",
    "            image_targets = targets[targets[:, 0] == i]\n",
    "            boxes = xywh2xyxy(image_targets[:, 2:6]).T\n",
    "            classes = image_targets[:, 1].astype('int')\n",
    "            gt = image_targets.shape[1] == 6  # ground truth if no conf column\n",
    "            conf = None if gt else image_targets[:, 6]  # check for confidence presence (gt vs pred)\n",
    "\n",
    "            boxes[[0, 2]] *= w\n",
    "            boxes[[0, 2]] += block_x\n",
    "            boxes[[1, 3]] *= h\n",
    "            boxes[[1, 3]] += block_y\n",
    "            for j, box in enumerate(boxes.T):\n",
    "                cls = int(classes[j])\n",
    "                color = color_lut[cls % len(color_lut)]\n",
    "                cls = names[cls] if names else cls\n",
    "                if gt or conf[j] > 0.3:  # 0.3 conf thresh\n",
    "                    label = '%s' % cls if gt else '%s %.1f' % (cls, conf[j])\n",
    "                    plot_one_box(box, mosaic, label=label, color=color, line_thickness=tl)\n",
    "\n",
    "        # Draw image filename labels\n",
    "        if paths is not None:\n",
    "            label = os.path.basename(paths[i])[:40]  # trim to 40 char\n",
    "            t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "            cv2.putText(mosaic, label, (block_x + 5, block_y + t_size[1] + 5), 0, tl / 3, [220, 220, 220], thickness=tf,\n",
    "                        lineType=cv2.LINE_AA)\n",
    "\n",
    "        # Image border\n",
    "        cv2.rectangle(mosaic, (block_x, block_y), (block_x + w, block_y + h), (255, 255, 255), thickness=3)\n",
    "\n",
    "    if fname is not None:\n",
    "        mosaic = cv2.resize(mosaic, (int(ns * w * 0.5), int(ns * h * 0.5)), interpolation=cv2.INTER_AREA)\n",
    "        cv2.imwrite(fname, cv2.cvtColor(mosaic, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    return mosaic\n",
    "\n",
    "\n",
    "def plot_lr_scheduler(optimizer, scheduler, epochs=300):\n",
    "    # Plot LR simulating training for full epochs\n",
    "    optimizer, scheduler = copy(optimizer), copy(scheduler)  # do not modify originals\n",
    "    y = []\n",
    "    for _ in range(epochs):\n",
    "        scheduler.step()\n",
    "        y.append(optimizer.param_groups[0]['lr'])\n",
    "    plt.plot(y, '.-', label='LR')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('LR')\n",
    "    plt.grid()\n",
    "    plt.xlim(0, epochs)\n",
    "    plt.ylim(0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('LR.png', dpi=200)\n",
    "\n",
    "\n",
    "def plot_test_txt():  # from utils.utils import *; plot_test()\n",
    "    # Plot test.txt histograms\n",
    "    x = np.loadtxt('test.txt', dtype=np.float32)\n",
    "    box = xyxy2xywh(x[:, :4])\n",
    "    cx, cy = box[:, 0], box[:, 1]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6), tight_layout=True)\n",
    "    ax.hist2d(cx, cy, bins=600, cmax=10, cmin=0)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.savefig('hist2d.png', dpi=300)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6), tight_layout=True)\n",
    "    ax[0].hist(cx, bins=600)\n",
    "    ax[1].hist(cy, bins=600)\n",
    "    plt.savefig('hist1d.png', dpi=200)\n",
    "\n",
    "\n",
    "def plot_targets_txt():  # from utils.utils import *; plot_targets_txt()\n",
    "    # Plot targets.txt histograms\n",
    "    x = np.loadtxt('targets.txt', dtype=np.float32).T\n",
    "    s = ['x targets', 'y targets', 'width targets', 'height targets']\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)\n",
    "    ax = ax.ravel()\n",
    "    for i in range(4):\n",
    "        ax[i].hist(x[i], bins=100, label='%.3g +/- %.3g' % (x[i].mean(), x[i].std()))\n",
    "        ax[i].legend()\n",
    "        ax[i].set_title(s[i])\n",
    "    plt.savefig('targets.jpg', dpi=200)\n",
    "\n",
    "\n",
    "def plot_study_txt(f='study.txt', x=None):  # from utils.utils import *; plot_study_txt()\n",
    "    # Plot study.txt generated by test.py\n",
    "    fig, ax = plt.subplots(2, 4, figsize=(10, 6), tight_layout=True)\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    fig2, ax2 = plt.subplots(1, 1, figsize=(8, 4), tight_layout=True)\n",
    "    for f in ['coco_study/study_coco_yolov5%s.txt' % x for x in ['s', 'm', 'l', 'x']]:\n",
    "        y = np.loadtxt(f, dtype=np.float32, usecols=[0, 1, 2, 3, 7, 8, 9], ndmin=2).T\n",
    "        x = np.arange(y.shape[1]) if x is None else np.array(x)\n",
    "        s = ['P', 'R', 'mAP@.5', 'mAP@.5:.95', 't_inference (ms/img)', 't_NMS (ms/img)', 't_total (ms/img)']\n",
    "        for i in range(7):\n",
    "            ax[i].plot(x, y[i], '.-', linewidth=2, markersize=8)\n",
    "            ax[i].set_title(s[i])\n",
    "\n",
    "        j = y[3].argmax() + 1\n",
    "        ax2.plot(y[6, :j], y[3, :j] * 1E2, '.-', linewidth=2, markersize=8,\n",
    "                 label=Path(f).stem.replace('study_coco_', '').replace('yolo', 'YOLO'))\n",
    "\n",
    "    ax2.plot(1E3 / np.array([209, 140, 97, 58, 35, 18]), [33.5, 39.1, 42.5, 45.9, 49., 50.5],\n",
    "             'k.-', linewidth=2, markersize=8, alpha=.25, label='EfficientDet')\n",
    "\n",
    "    ax2.grid()\n",
    "    ax2.set_xlim(0, 30)\n",
    "    ax2.set_ylim(28, 50)\n",
    "    ax2.set_yticks(np.arange(30, 55, 5))\n",
    "    ax2.set_xlabel('GPU Speed (ms/img)')\n",
    "    ax2.set_ylabel('COCO AP val')\n",
    "    ax2.legend(loc='lower right')\n",
    "    plt.savefig('study_mAP_latency.png', dpi=300)\n",
    "    plt.savefig(f.replace('.txt', '.png'), dpi=200)\n",
    "\n",
    "\n",
    "def plot_labels(labels):\n",
    "    # plot dataset labels\n",
    "    c, b = labels[:, 0], labels[:, 1:].transpose()  # classees, boxes\n",
    "\n",
    "    def hist2d(x, y, n=100):\n",
    "        xedges, yedges = np.linspace(x.min(), x.max(), n), np.linspace(y.min(), y.max(), n)\n",
    "        hist, xedges, yedges = np.histogram2d(x, y, (xedges, yedges))\n",
    "        xidx = np.clip(np.digitize(x, xedges) - 1, 0, hist.shape[0] - 1)\n",
    "        yidx = np.clip(np.digitize(y, yedges) - 1, 0, hist.shape[1] - 1)\n",
    "        return np.log(hist[xidx, yidx])\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)\n",
    "    ax = ax.ravel()\n",
    "    ax[0].hist(c, bins=int(c.max() + 1))\n",
    "    ax[0].set_xlabel('classes')\n",
    "    ax[1].scatter(b[0], b[1], c=hist2d(b[0], b[1], 90), cmap='jet')\n",
    "    ax[1].set_xlabel('x')\n",
    "    ax[1].set_ylabel('y')\n",
    "    ax[2].scatter(b[2], b[3], c=hist2d(b[2], b[3], 90), cmap='jet')\n",
    "    ax[2].set_xlabel('width')\n",
    "    ax[2].set_ylabel('height')\n",
    "    plt.savefig('labels.png', dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_evolution_results(hyp):  # from utils.utils import *; plot_evolution_results(hyp)\n",
    "    # Plot hyperparameter evolution results in evolve.txt\n",
    "    x = np.loadtxt('evolve.txt', ndmin=2)\n",
    "    f = fitness(x)\n",
    "    # weights = (f - f.min()) ** 2  # for weighted results\n",
    "    plt.figure(figsize=(12, 10), tight_layout=True)\n",
    "    matplotlib.rc('font', **{'size': 8})\n",
    "    for i, (k, v) in enumerate(hyp.items()):\n",
    "        y = x[:, i + 7]\n",
    "        # mu = (y * weights).sum() / weights.sum()  # best weighted result\n",
    "        mu = y[f.argmax()]  # best single result\n",
    "        plt.subplot(4, 5, i + 1)\n",
    "        plt.plot(mu, f.max(), 'o', markersize=10)\n",
    "        plt.plot(y, f, '.')\n",
    "        plt.title('%s = %.3g' % (k, mu), fontdict={'size': 9})  # limit to 40 characters\n",
    "        print('%15s: %.3g' % (k, mu))\n",
    "    plt.savefig('evolve.png', dpi=200)\n",
    "\n",
    "\n",
    "def plot_results_overlay(start=0, stop=0):  # from utils.utils import *; plot_results_overlay()\n",
    "    # Plot training 'results*.txt', overlaying train and val losses\n",
    "    s = ['train', 'train', 'train', 'Precision', 'mAP@0.5', 'val', 'val', 'val', 'Recall', 'mAP@0.5:0.95']  # legends\n",
    "    t = ['GIoU', 'Objectness', 'Classification', 'P-R', 'mAP-F1']  # titles\n",
    "    for f in sorted(glob.glob('results*.txt') + glob.glob('../../Downloads/results*.txt')):\n",
    "        results = np.loadtxt(f, usecols=[2, 3, 4, 8, 9, 12, 13, 14, 10, 11], ndmin=2).T\n",
    "        n = results.shape[1]  # number of rows\n",
    "        x = range(start, min(stop, n) if stop else n)\n",
    "        fig, ax = plt.subplots(1, 5, figsize=(14, 3.5), tight_layout=True)\n",
    "        ax = ax.ravel()\n",
    "        for i in range(5):\n",
    "            for j in [i, i + 5]:\n",
    "                y = results[j, x]\n",
    "                ax[i].plot(x, y, marker='.', label=s[j])\n",
    "                # y_smooth = butter_lowpass_filtfilt(y)\n",
    "                # ax[i].plot(x, np.gradient(y_smooth), marker='.', label=s[j])\n",
    "\n",
    "            ax[i].set_title(t[i])\n",
    "            ax[i].legend()\n",
    "            ax[i].set_ylabel(f) if i == 0 else None  # add filename\n",
    "        fig.savefig(f.replace('.txt', '.png'), dpi=200)\n",
    "\n",
    "\n",
    "def plot_results(start=0, stop=0, bucket='', id=(), labels=()):  # from utils.utils import *; plot_results()\n",
    "    # Plot training 'results*.txt' as seen in https://github.com/ultralytics/yolov5#reproduce-our-training\n",
    "    fig, ax = plt.subplots(2, 5, figsize=(12, 6))\n",
    "    ax = ax.ravel()\n",
    "    s = ['GIoU', 'Objectness', 'Classification', 'Precision', 'Recall',\n",
    "         'val GIoU', 'val Objectness', 'val Classification', 'mAP@0.5', 'mAP@0.5:0.95']\n",
    "    if bucket:\n",
    "        os.system('rm -rf storage.googleapis.com')\n",
    "        files = ['https://storage.googleapis.com/%s/results%g.txt' % (bucket, x) for x in id]\n",
    "    else:\n",
    "        files = glob.glob('results*.txt') + glob.glob('../../Downloads/results*.txt')\n",
    "    for fi, f in enumerate(files):\n",
    "        try:\n",
    "            results = np.loadtxt(f, usecols=[2, 3, 4, 8, 9, 12, 13, 14, 10, 11], ndmin=2).T\n",
    "            n = results.shape[1]  # number of rows\n",
    "            x = range(start, min(stop, n) if stop else n)\n",
    "            for i in range(10):\n",
    "                y = results[i, x]\n",
    "                if i in [0, 1, 2, 5, 6, 7]:\n",
    "                    y[y == 0] = np.nan  # dont show zero loss values\n",
    "                    # y /= y[0]  # normalize\n",
    "                label = labels[fi] if len(labels) else Path(f).stem\n",
    "                ax[i].plot(x, y, marker='.', label=label, linewidth=2, markersize=8)\n",
    "                ax[i].set_title(s[i])\n",
    "                # if i in [5, 6, 7]:  # share train and val loss y axes\n",
    "                #     ax[i].get_shared_y_axes().join(ax[i], ax[i - 5])\n",
    "        except:\n",
    "            print('Warning: Plotting error for %s, skipping file' % f)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    ax[1].legend()\n",
    "    fig.savefig('results.png', dpi=200)\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def attempt_download(weights):\n",
    "    # Attempt to download pretrained weights if not found locally\n",
    "    weights = weights.strip()\n",
    "    msg = weights + ' missing, try downloading from https://drive.google.com/drive/folders/1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J'\n",
    "\n",
    "    r = 1\n",
    "    if len(weights) > 0 and not os.path.isfile(weights):\n",
    "        d = {'yolov3-spp.pt': '1mM67oNw4fZoIOL1c8M3hHmj66d8e-ni_',  # yolov3-spp.yaml\n",
    "             'yolov5s.pt': '1R5T6rIyy3lLwgFXNms8whc-387H0tMQO',  # yolov5s.yaml\n",
    "             'yolov5m.pt': '1vobuEExpWQVpXExsJ2w-Mbf3HJjWkQJr',  # yolov5m.yaml\n",
    "             'yolov5l.pt': '1hrlqD1Wdei7UT4OgT785BEk1JwnSvNEV',  # yolov5l.yaml\n",
    "             'yolov5x.pt': '1mM8aZJlWTxOg7BZJvNUMrTnA2AbeCVzS',  # yolov5x.yaml\n",
    "             }\n",
    "\n",
    "        file = Path(weights).name\n",
    "        if file in d:\n",
    "            r = gdrive_download(id=d[file], name=weights)\n",
    "\n",
    "        if not (r == 0 and os.path.exists(weights) and os.path.getsize(weights) > 1E6):  # weights exist and > 1MB\n",
    "            os.remove(weights) if os.path.exists(weights) else None  # remove partial downloads\n",
    "            s = \"curl -L -o %s 'https://storage.googleapis.com/ultralytics/yolov5/ckpt/%s'\" % (weights, file)\n",
    "            r = os.system(s)  # execute, capture return values\n",
    "\n",
    "            # Error check\n",
    "            if not (r == 0 and os.path.exists(weights) and os.path.getsize(weights) > 1E6):  # weights exist and > 1MB\n",
    "                os.remove(weights) if os.path.exists(weights) else None  # remove partial downloads\n",
    "                raise Exception(msg)\n",
    "\n",
    "\n",
    "def gdrive_download(id='1HaXkef9z6y5l4vUnCYgdmEAj61c6bfWO', name='coco.zip'):\n",
    "    # https://gist.github.com/tanaikech/f0f2d122e05bf5f971611258c22c110f\n",
    "    # Downloads a file from Google Drive, accepting presented query\n",
    "    # from utils.google_utils import *; gdrive_download()\n",
    "    t = time.time()\n",
    "\n",
    "    print('Downloading https://drive.google.com/uc?export=download&id=%s as %s... ' % (id, name), end='')\n",
    "    os.remove(name) if os.path.exists(name) else None  # remove existing\n",
    "    os.remove('cookie') if os.path.exists('cookie') else None\n",
    "\n",
    "    # Attempt file download\n",
    "    os.system(\"curl -c ./cookie -s -L \\\"https://drive.google.com/uc?export=download&id=%s\\\" > /dev/null\" % id)\n",
    "    if os.path.exists('cookie'):  # large file\n",
    "        s = \"curl -Lb ./cookie \\\"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=%s\\\" -o %s\" % (\n",
    "            id, name)\n",
    "    else:  # small file\n",
    "        s = \"curl -s -L -o %s 'https://drive.google.com/uc?export=download&id=%s'\" % (name, id)\n",
    "    r = os.system(s)  # execute, capture return values\n",
    "    os.remove('cookie') if os.path.exists('cookie') else None\n",
    "\n",
    "    # Error check\n",
    "    if r != 0:\n",
    "        os.remove(name) if os.path.exists(name) else None  # remove partial\n",
    "        print('Download error ')  # raise Exception('Download error')\n",
    "        return r\n",
    "\n",
    "    # Unzip if archive\n",
    "    if name.endswith('.zip'):\n",
    "        print('unzipping... ', end='')\n",
    "        os.system('unzip -q %s' % name)  # unzip\n",
    "        os.remove(name)  # remove zip to free space\n",
    "\n",
    "    print('Done (%.1fs)' % (time.time() - t))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASETS #\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ExifTags\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "help_url = 'https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data'\n",
    "img_formats = ['.bmp', '.jpg', '.jpeg', '.png', '.tif', '.dng']\n",
    "vid_formats = ['.mov', '.avi', '.mp4', '.mpg', '.mpeg', '.m4v', '.wmv', '.mkv']\n",
    "\n",
    "# Get orientation exif tag\n",
    "for orientation in ExifTags.TAGS.keys():\n",
    "    if ExifTags.TAGS[orientation] == 'Orientation':\n",
    "        break\n",
    "\n",
    "\n",
    "def exif_size(img):\n",
    "    # Returns exif-corrected PIL size\n",
    "    s = img.size  # (width, height)\n",
    "    try:\n",
    "        rotation = dict(img._getexif().items())[orientation]\n",
    "        if rotation == 6:  # rotation 270\n",
    "            s = (s[1], s[0])\n",
    "        elif rotation == 8:  # rotation 90\n",
    "            s = (s[1], s[0])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "class LoadImages:  # for inference\n",
    "    def __init__(self, path, img_size=416):\n",
    "        path = str(Path(path))  # os-agnostic\n",
    "        files = []\n",
    "        if os.path.isdir(path):\n",
    "            files = sorted(glob.glob(os.path.join(path, '*.*')))\n",
    "        elif os.path.isfile(path):\n",
    "            files = [path]\n",
    "\n",
    "        images = [x for x in files if os.path.splitext(x)[-1].lower() in img_formats]\n",
    "        videos = [x for x in files if os.path.splitext(x)[-1].lower() in vid_formats]\n",
    "        nI, nV = len(images), len(videos)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.files = images + videos\n",
    "        self.nF = nI + nV  # number of files\n",
    "        self.video_flag = [False] * nI + [True] * nV\n",
    "        self.mode = 'images'\n",
    "        if any(videos):\n",
    "            self.new_video(videos[0])  # new video\n",
    "        else:\n",
    "            self.cap = None\n",
    "        assert self.nF > 0, 'No images or videos found in %s. Supported formats are:\\nimages: %s\\nvideos: %s' % \\\n",
    "                            (path, img_formats, vid_formats)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.count == self.nF:\n",
    "            raise StopIteration\n",
    "        path = self.files[self.count]\n",
    "\n",
    "        if self.video_flag[self.count]:\n",
    "            # Read video\n",
    "            self.mode = 'video'\n",
    "            ret_val, img0 = self.cap.read()\n",
    "            if not ret_val:\n",
    "                self.count += 1\n",
    "                self.cap.release()\n",
    "                if self.count == self.nF:  # last video\n",
    "                    raise StopIteration\n",
    "                else:\n",
    "                    path = self.files[self.count]\n",
    "                    self.new_video(path)\n",
    "                    ret_val, img0 = self.cap.read()\n",
    "\n",
    "            self.frame += 1\n",
    "            print('video %g/%g (%g/%g) %s: ' % (self.count + 1, self.nF, self.frame, self.nframes, path), end='')\n",
    "\n",
    "        else:\n",
    "            # Read image\n",
    "            self.count += 1\n",
    "            img0 = cv2.imread(path)  # BGR\n",
    "            assert img0 is not None, 'Image Not Found ' + path\n",
    "            print('image %g/%g %s: ' % (self.count, self.nF, path), end='')\n",
    "\n",
    "        # Padded resize\n",
    "        img = letterbox(img0, new_shape=self.img_size)[0]\n",
    "\n",
    "        # Convert\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        # cv2.imwrite(path + '.letterbox.jpg', 255 * img.transpose((1, 2, 0))[:, :, ::-1])  # save letterbox image\n",
    "        return path, img, img0, self.cap\n",
    "\n",
    "    def new_video(self, path):\n",
    "        self.frame = 0\n",
    "        self.cap = cv2.VideoCapture(path)\n",
    "        self.nframes = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nF  # number of files\n",
    "\n",
    "\n",
    "class LoadWebcam:  # for inference\n",
    "    def __init__(self, pipe=0, img_size=416):\n",
    "        self.img_size = img_size\n",
    "\n",
    "        if pipe == '0':\n",
    "            pipe = 0  # local camera\n",
    "        # pipe = 'rtsp://192.168.1.64/1'  # IP camera\n",
    "        # pipe = 'rtsp://username:password@192.168.1.64/1'  # IP camera with login\n",
    "        # pipe = 'rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa'  # IP traffic camera\n",
    "        # pipe = 'http://wmccpinetop.axiscam.net/mjpg/video.mjpg'  # IP golf camera\n",
    "\n",
    "        # https://answers.opencv.org/question/215996/changing-gstreamer-pipeline-to-opencv-in-pythonsolved/\n",
    "        # pipe = '\"rtspsrc location=\"rtsp://username:password@192.168.1.64/1\" latency=10 ! appsink'  # GStreamer\n",
    "\n",
    "        # https://answers.opencv.org/question/200787/video-acceleration-gstremer-pipeline-in-videocapture/\n",
    "        # https://stackoverflow.com/questions/54095699/install-gstreamer-support-for-opencv-python-package  # install help\n",
    "        # pipe = \"rtspsrc location=rtsp://root:root@192.168.0.91:554/axis-media/media.amp?videocodec=h264&resolution=3840x2160 protocols=GST_RTSP_LOWER_TRANS_TCP ! rtph264depay ! queue ! vaapih264dec ! videoconvert ! appsink\"  # GStreamer\n",
    "\n",
    "        self.pipe = pipe\n",
    "        self.cap = cv2.VideoCapture(pipe)  # video capture object\n",
    "        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 3)  # set buffer size\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = -1\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        self.count += 1\n",
    "        if cv2.waitKey(1) == ord('q'):  # q to quit\n",
    "            self.cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            raise StopIteration\n",
    "\n",
    "        # Read frame\n",
    "        if self.pipe == 0:  # local camera\n",
    "            ret_val, img0 = self.cap.read()\n",
    "            img0 = cv2.flip(img0, 1)  # flip left-right\n",
    "        else:  # IP camera\n",
    "            n = 0\n",
    "            while True:\n",
    "                n += 1\n",
    "                self.cap.grab()\n",
    "                if n % 30 == 0:  # skip frames\n",
    "                    ret_val, img0 = self.cap.retrieve()\n",
    "                    if ret_val:\n",
    "                        break\n",
    "\n",
    "        # Print\n",
    "        assert ret_val, 'Camera Error %s' % self.pipe\n",
    "        img_path = 'webcam.jpg'\n",
    "        print('webcam %g: ' % self.count, end='')\n",
    "\n",
    "        # Padded resize\n",
    "        img = letterbox(img0, new_shape=self.img_size)[0]\n",
    "\n",
    "        # Convert\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        return img_path, img, img0, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return 0\n",
    "\n",
    "\n",
    "class LoadStreams:  # multiple IP or RTSP cameras\n",
    "    def __init__(self, sources='streams.txt', img_size=416):\n",
    "        self.mode = 'images'\n",
    "        self.img_size = img_size\n",
    "\n",
    "        if os.path.isfile(sources):\n",
    "            with open(sources, 'r') as f:\n",
    "                sources = [x.strip() for x in f.read().splitlines() if len(x.strip())]\n",
    "        else:\n",
    "            sources = [sources]\n",
    "\n",
    "        n = len(sources)\n",
    "        self.imgs = [None] * n\n",
    "        self.sources = sources\n",
    "        for i, s in enumerate(sources):\n",
    "            # Start the thread to read frames from the video stream\n",
    "            print('%g/%g: %s... ' % (i + 1, n, s), end='')\n",
    "            cap = cv2.VideoCapture(0 if s == '0' else s)\n",
    "            assert cap.isOpened(), 'Failed to open %s' % s\n",
    "            w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS) % 100\n",
    "            _, self.imgs[i] = cap.read()  # guarantee first frame\n",
    "            thread = Thread(target=self.update, args=([i, cap]), daemon=True)\n",
    "            print(' success (%gx%g at %.2f FPS).' % (w, h, fps))\n",
    "            thread.start()\n",
    "        print('')  # newline\n",
    "\n",
    "        # check for common shapes\n",
    "        s = np.stack([letterbox(x, new_shape=self.img_size)[0].shape for x in self.imgs], 0)  # inference shapes\n",
    "        self.rect = np.unique(s, axis=0).shape[0] == 1  # rect inference if all shapes equal\n",
    "        if not self.rect:\n",
    "            print('WARNING: Different stream shapes detected. For optimal performance supply similarly-shaped streams.')\n",
    "\n",
    "    def update(self, index, cap):\n",
    "        # Read next stream frame in a daemon thread\n",
    "        n = 0\n",
    "        while cap.isOpened():\n",
    "            n += 1\n",
    "            # _, self.imgs[index] = cap.read()\n",
    "            cap.grab()\n",
    "            if n == 4:  # read every 4th frame\n",
    "                _, self.imgs[index] = cap.retrieve()\n",
    "                n = 0\n",
    "            time.sleep(0.01)  # wait time\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = -1\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        self.count += 1\n",
    "        img0 = self.imgs.copy()\n",
    "        if cv2.waitKey(1) == ord('q'):  # q to quit\n",
    "            cv2.destroyAllWindows()\n",
    "            raise StopIteration\n",
    "\n",
    "        # Letterbox\n",
    "        img = [letterbox(x, new_shape=self.img_size, auto=self.rect)[0] for x in img0]\n",
    "\n",
    "        # Stack\n",
    "        img = np.stack(img, 0)\n",
    "\n",
    "        # Convert\n",
    "        img = img[:, :, :, ::-1].transpose(0, 3, 1, 2)  # BGR to RGB, to bsx3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        return self.sources, img, img0, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return 0  # 1E12 frames = 32 streams at 30 FPS for 30 years\n",
    "\n",
    "\n",
    "class LoadImagesAndLabels(Dataset):  # for training/testing\n",
    "    def __init__(self, path, img_size=416, batch_size=16, augment=False, hyp=None, rect=False, image_weights=False,\n",
    "                 cache_images=False, single_cls=False, stride=32, pad=0.0):\n",
    "        try:\n",
    "            path = str(Path(path))  # os-agnostic\n",
    "            parent = str(Path(path).parent) + os.sep\n",
    "            if os.path.isfile(path):  # file\n",
    "                with open(path, 'r') as f:\n",
    "                    f = f.read().splitlines()\n",
    "                    f = [x.replace('./', parent) if x.startswith('./') else x for x in f]  # local to global path\n",
    "            elif os.path.isdir(path):  # folder\n",
    "                f = glob.iglob(path + os.sep + '*.*')\n",
    "            else:\n",
    "                raise Exception('%s does not exist' % path)\n",
    "            self.img_files = [x.replace('/', os.sep) for x in f if os.path.splitext(x)[-1].lower() in img_formats]\n",
    "        except:\n",
    "            raise Exception('Error loading data from %s. See %s' % (path, help_url))\n",
    "\n",
    "        n = len(self.img_files)\n",
    "        assert n > 0, 'No images found in %s. See %s' % (path, help_url)\n",
    "        bi = np.floor(np.arange(n) / batch_size).astype(np.int)  # batch index\n",
    "        nb = bi[-1] + 1  # number of batches\n",
    "\n",
    "        self.n = n  # number of images\n",
    "        self.batch = bi  # batch index of image\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "        self.hyp = hyp\n",
    "        self.image_weights = image_weights\n",
    "        self.rect = False if image_weights else rect\n",
    "        self.mosaic = self.augment and not self.rect  # load 4 images at a time into a mosaic (only during training)\n",
    "\n",
    "        # Define labels\n",
    "        self.label_files = [x.replace('images', 'labels').replace(os.path.splitext(x)[-1], '.txt')\n",
    "                            for x in self.img_files]\n",
    "\n",
    "        # Read image shapes (wh)\n",
    "        sp = path.replace('.txt', '') + '.shapes'  # shapefile path\n",
    "        try:\n",
    "            with open(sp, 'r') as f:  # read existing shapefile\n",
    "                s = [x.split() for x in f.read().splitlines()]\n",
    "                assert len(s) == n, 'Shapefile out of sync'\n",
    "        except:\n",
    "            s = [exif_size(Image.open(f)) for f in tqdm(self.img_files, desc='Reading image shapes')]\n",
    "            np.savetxt(sp, s, fmt='%g')  # overwrites existing (if any)\n",
    "\n",
    "        self.shapes = np.array(s, dtype=np.float64)\n",
    "\n",
    "        # Rectangular Training  https://github.com/ultralytics/yolov3/issues/232\n",
    "        if self.rect:\n",
    "            # Sort by aspect ratio\n",
    "            s = self.shapes  # wh\n",
    "            ar = s[:, 1] / s[:, 0]  # aspect ratio\n",
    "            irect = ar.argsort()\n",
    "            self.img_files = [self.img_files[i] for i in irect]\n",
    "            self.label_files = [self.label_files[i] for i in irect]\n",
    "            self.shapes = s[irect]  # wh\n",
    "            ar = ar[irect]\n",
    "\n",
    "            # Set training image shapes\n",
    "            shapes = [[1, 1]] * nb\n",
    "            for i in range(nb):\n",
    "                ari = ar[bi == i]\n",
    "                mini, maxi = ari.min(), ari.max()\n",
    "                if maxi < 1:\n",
    "                    shapes[i] = [maxi, 1]\n",
    "                elif mini > 1:\n",
    "                    shapes[i] = [1, 1 / mini]\n",
    "\n",
    "            self.batch_shapes = np.ceil(np.array(shapes) * img_size / stride + pad).astype(np.int) * stride\n",
    "\n",
    "        # Cache labels\n",
    "        self.imgs = [None] * n\n",
    "        self.labels = [np.zeros((0, 5), dtype=np.float32)] * n\n",
    "        create_datasubset, extract_bounding_boxes, labels_loaded = False, False, False\n",
    "        nm, nf, ne, ns, nd = 0, 0, 0, 0, 0  # number missing, found, empty, datasubset, duplicate\n",
    "        np_labels_path = str(Path(self.label_files[0]).parent) + '.npy'  # saved labels in *.npy file\n",
    "        if os.path.isfile(np_labels_path):\n",
    "            s = np_labels_path  # print string\n",
    "            x = np.load(np_labels_path, allow_pickle=True)\n",
    "            if len(x) == n:\n",
    "                self.labels = x\n",
    "                labels_loaded = True\n",
    "        else:\n",
    "            s = path.replace('images', 'labels')\n",
    "\n",
    "        pbar = tqdm(self.label_files)\n",
    "        for i, file in enumerate(pbar):\n",
    "            if labels_loaded:\n",
    "                l = self.labels[i]\n",
    "                # np.savetxt(file, l, '%g')  # save *.txt from *.npy file\n",
    "            else:\n",
    "                try:\n",
    "                    with open(file, 'r') as f:\n",
    "                        l = np.array([x.split() for x in f.read().splitlines()], dtype=np.float32)\n",
    "                except:\n",
    "                    nm += 1  # print('missing labels for image %s' % self.img_files[i])  # file missing\n",
    "                    continue\n",
    "\n",
    "            if l.shape[0]:\n",
    "                assert l.shape[1] == 5, '> 5 label columns: %s' % file\n",
    "                assert (l >= 0).all(), 'negative labels: %s' % file\n",
    "                assert (l[:, 1:] <= 1).all(), 'non-normalized or out of bounds coordinate labels: %s' % file\n",
    "                if np.unique(l, axis=0).shape[0] < l.shape[0]:  # duplicate rows\n",
    "                    nd += 1  # print('WARNING: duplicate rows in %s' % self.label_files[i])  # duplicate rows\n",
    "                if single_cls:\n",
    "                    l[:, 0] = 0  # force dataset into single-class mode\n",
    "                self.labels[i] = l\n",
    "                nf += 1  # file found\n",
    "\n",
    "                # Create subdataset (a smaller dataset)\n",
    "                if create_datasubset and ns < 1E4:\n",
    "                    if ns == 0:\n",
    "                        create_folder(path='./datasubset')\n",
    "                        os.makedirs('./datasubset/images')\n",
    "                    exclude_classes = 43\n",
    "                    if exclude_classes not in l[:, 0]:\n",
    "                        ns += 1\n",
    "                        # shutil.copy(src=self.img_files[i], dst='./datasubset/images/')  # copy image\n",
    "                        with open('./datasubset/images.txt', 'a') as f:\n",
    "                            f.write(self.img_files[i] + '\\n')\n",
    "\n",
    "                # Extract object detection boxes for a second stage classifier\n",
    "                if extract_bounding_boxes:\n",
    "                    p = Path(self.img_files[i])\n",
    "                    img = cv2.imread(str(p))\n",
    "                    h, w = img.shape[:2]\n",
    "                    for j, x in enumerate(l):\n",
    "                        f = '%s%sclassifier%s%g_%g_%s' % (p.parent.parent, os.sep, os.sep, x[0], j, p.name)\n",
    "                        if not os.path.exists(Path(f).parent):\n",
    "                            os.makedirs(Path(f).parent)  # make new output folder\n",
    "\n",
    "                        b = x[1:] * [w, h, w, h]  # box\n",
    "                        b[2:] = b[2:].max()  # rectangle to square\n",
    "                        b[2:] = b[2:] * 1.3 + 30  # pad\n",
    "                        b = xywh2xyxy(b.reshape(-1, 4)).ravel().astype(np.int)\n",
    "\n",
    "                        b[[0, 2]] = np.clip(b[[0, 2]], 0, w)  # clip boxes outside of image\n",
    "                        b[[1, 3]] = np.clip(b[[1, 3]], 0, h)\n",
    "                        assert cv2.imwrite(f, img[b[1]:b[3], b[0]:b[2]]), 'Failure extracting classifier boxes'\n",
    "            else:\n",
    "                ne += 1  # print('empty labels for image %s' % self.img_files[i])  # file empty\n",
    "                # os.system(\"rm '%s' '%s'\" % (self.img_files[i], self.label_files[i]))  # remove\n",
    "\n",
    "            pbar.desc = 'Caching labels %s (%g found, %g missing, %g empty, %g duplicate, for %g images)' % (\n",
    "                s, nf, nm, ne, nd, n)\n",
    "        assert nf > 0 or n == 20288, 'No labels found in %s. See %s' % (os.path.dirname(file) + os.sep, help_url)\n",
    "        if not labels_loaded and n > 1000:\n",
    "            print('Saving labels to %s for faster future loading' % np_labels_path)\n",
    "            np.save(np_labels_path, self.labels)  # save for next time\n",
    "\n",
    "        # Cache images into memory for faster training (WARNING: large datasets may exceed system RAM)\n",
    "        if cache_images:  # if training\n",
    "            gb = 0  # Gigabytes of cached images\n",
    "            pbar = tqdm(range(len(self.img_files)), desc='Caching images')\n",
    "            self.img_hw0, self.img_hw = [None] * n, [None] * n\n",
    "            for i in pbar:  # max 10k images\n",
    "                self.imgs[i], self.img_hw0[i], self.img_hw[i] = load_image(self, i)  # img, hw_original, hw_resized\n",
    "                gb += self.imgs[i].nbytes\n",
    "                pbar.desc = 'Caching images (%.1fGB)' % (gb / 1E9)\n",
    "\n",
    "        # Detect corrupted images https://medium.com/joelthchao/programmatically-detect-corrupted-image-8c1b2006c3d3\n",
    "        detect_corrupted_images = False\n",
    "        if detect_corrupted_images:\n",
    "            from skimage import io  # conda install -c conda-forge scikit-image\n",
    "            for file in tqdm(self.img_files, desc='Detecting corrupted images'):\n",
    "                try:\n",
    "                    _ = io.imread(file)\n",
    "                except:\n",
    "                    print('Corrupted image detected: %s' % file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    # def __iter__(self):\n",
    "    #     self.count = -1\n",
    "    #     print('ran dataset iter')\n",
    "    #     #self.shuffled_vector = np.random.permutation(self.nF) if self.augment else np.arange(self.nF)\n",
    "    #     return self\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.image_weights:\n",
    "            index = self.indices[index]\n",
    "\n",
    "        hyp = self.hyp\n",
    "        if self.mosaic:\n",
    "            # Load mosaic\n",
    "            img, labels = load_mosaic(self, index)\n",
    "            shapes = None\n",
    "\n",
    "        else:\n",
    "            # Load image\n",
    "            img, (h0, w0), (h, w) = load_image(self, index)\n",
    "\n",
    "            # Letterbox\n",
    "            shape = self.batch_shapes[self.batch[index]] if self.rect else self.img_size  # final letterboxed shape\n",
    "            img, ratio, pad = letterbox(img, shape, auto=False, scaleup=self.augment)\n",
    "            shapes = (h0, w0), ((h / h0, w / w0), pad)  # for COCO mAP rescaling\n",
    "\n",
    "            # Load labels\n",
    "            labels = []\n",
    "            x = self.labels[index]\n",
    "            if x.size > 0:\n",
    "                # Normalized xywh to pixel xyxy format\n",
    "                labels = x.copy()\n",
    "                labels[:, 1] = ratio[0] * w * (x[:, 1] - x[:, 3] / 2) + pad[0]  # pad width\n",
    "                labels[:, 2] = ratio[1] * h * (x[:, 2] - x[:, 4] / 2) + pad[1]  # pad height\n",
    "                labels[:, 3] = ratio[0] * w * (x[:, 1] + x[:, 3] / 2) + pad[0]\n",
    "                labels[:, 4] = ratio[1] * h * (x[:, 2] + x[:, 4] / 2) + pad[1]\n",
    "\n",
    "        if self.augment:\n",
    "            # Augment imagespace\n",
    "            if not self.mosaic:\n",
    "                img, labels = random_affine(img, labels,\n",
    "                                            degrees=hyp['degrees'],\n",
    "                                            translate=hyp['translate'],\n",
    "                                            scale=hyp['scale'],\n",
    "                                            shear=hyp['shear'])\n",
    "\n",
    "            # Augment colorspace\n",
    "            augment_hsv(img, hgain=hyp['hsv_h'], sgain=hyp['hsv_s'], vgain=hyp['hsv_v'])\n",
    "\n",
    "            # Apply cutouts\n",
    "            # if random.random() < 0.9:\n",
    "            #     labels = cutout(img, labels)\n",
    "\n",
    "        nL = len(labels)  # number of labels\n",
    "        if nL:\n",
    "            # convert xyxy to xywh\n",
    "            labels[:, 1:5] = xyxy2xywh(labels[:, 1:5])\n",
    "\n",
    "            # Normalize coordinates 0 - 1\n",
    "            labels[:, [2, 4]] /= img.shape[0]  # height\n",
    "            labels[:, [1, 3]] /= img.shape[1]  # width\n",
    "\n",
    "        if self.augment:\n",
    "            # random left-right flip\n",
    "            lr_flip = True\n",
    "            if lr_flip and random.random() < 0.5:\n",
    "                img = np.fliplr(img)\n",
    "                if nL:\n",
    "                    labels[:, 1] = 1 - labels[:, 1]\n",
    "\n",
    "            # random up-down flip\n",
    "            ud_flip = False\n",
    "            if ud_flip and random.random() < 0.5:\n",
    "                img = np.flipud(img)\n",
    "                if nL:\n",
    "                    labels[:, 2] = 1 - labels[:, 2]\n",
    "\n",
    "        labels_out = torch.zeros((nL, 6))\n",
    "        if nL:\n",
    "            labels_out[:, 1:] = torch.from_numpy(labels)\n",
    "\n",
    "        # Convert\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        return torch.from_numpy(img), labels_out, self.img_files[index], shapes\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        img, label, path, shapes = zip(*batch)  # transposed\n",
    "        for i, l in enumerate(label):\n",
    "            l[:, 0] = i  # add target image index for build_targets()\n",
    "        return torch.stack(img, 0), torch.cat(label, 0), path, shapes\n",
    "\n",
    "\n",
    "def load_image(self, index):\n",
    "    # loads 1 image from dataset, returns img, original hw, resized hw\n",
    "    img = self.imgs[index]\n",
    "    if img is None:  # not cached\n",
    "        path = self.img_files[index]\n",
    "        img = cv2.imread(path)  # BGR\n",
    "        assert img is not None, 'Image Not Found ' + path\n",
    "        h0, w0 = img.shape[:2]  # orig hw\n",
    "        r = self.img_size / max(h0, w0)  # resize image to img_size\n",
    "        if r != 1:  # always resize down, only resize up if training with augmentation\n",
    "            interp = cv2.INTER_AREA if r < 1 and not self.augment else cv2.INTER_LINEAR\n",
    "            img = cv2.resize(img, (int(w0 * r), int(h0 * r)), interpolation=interp)\n",
    "        return img, (h0, w0), img.shape[:2]  # img, hw_original, hw_resized\n",
    "    else:\n",
    "        return self.imgs[index], self.img_hw0[index], self.img_hw[index]  # img, hw_original, hw_resized\n",
    "\n",
    "\n",
    "def augment_hsv(img, hgain=0.5, sgain=0.5, vgain=0.5):\n",
    "    r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains\n",
    "    hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\n",
    "    dtype = img.dtype  # uint8\n",
    "\n",
    "    x = np.arange(0, 256, dtype=np.int16)\n",
    "    lut_hue = ((x * r[0]) % 180).astype(dtype)\n",
    "    lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n",
    "    lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n",
    "\n",
    "    img_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val))).astype(dtype)\n",
    "    cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR, dst=img)  # no return needed\n",
    "\n",
    "    # Histogram equalization\n",
    "    # if random.random() < 0.2:\n",
    "    #     for i in range(3):\n",
    "    #         img[:, :, i] = cv2.equalizeHist(img[:, :, i])\n",
    "\n",
    "\n",
    "def load_mosaic(self, index):\n",
    "    # loads images in a mosaic\n",
    "\n",
    "    labels4 = []\n",
    "    s = self.img_size\n",
    "    xc, yc = [int(random.uniform(s * 0.5, s * 1.5)) for _ in range(2)]  # mosaic center x, y\n",
    "    indices = [index] + [random.randint(0, len(self.labels) - 1) for _ in range(3)]  # 3 additional image indices\n",
    "    for i, index in enumerate(indices):\n",
    "        # Load image\n",
    "        img, _, (h, w) = load_image(self, index)\n",
    "\n",
    "        # place img in img4\n",
    "        if i == 0:  # top left\n",
    "            img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n",
    "            x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n",
    "            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n",
    "        elif i == 1:  # top right\n",
    "            x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n",
    "            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n",
    "        elif i == 2:  # bottom left\n",
    "            x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n",
    "            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n",
    "        elif i == 3:  # bottom right\n",
    "            x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n",
    "            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n",
    "\n",
    "        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n",
    "        padw = x1a - x1b\n",
    "        padh = y1a - y1b\n",
    "\n",
    "        # Labels\n",
    "        x = self.labels[index]\n",
    "        labels = x.copy()\n",
    "        if x.size > 0:  # Normalized xywh to pixel xyxy format\n",
    "            labels[:, 1] = w * (x[:, 1] - x[:, 3] / 2) + padw\n",
    "            labels[:, 2] = h * (x[:, 2] - x[:, 4] / 2) + padh\n",
    "            labels[:, 3] = w * (x[:, 1] + x[:, 3] / 2) + padw\n",
    "            labels[:, 4] = h * (x[:, 2] + x[:, 4] / 2) + padh\n",
    "        labels4.append(labels)\n",
    "\n",
    "    # Concat/clip labels\n",
    "    if len(labels4):\n",
    "        labels4 = np.concatenate(labels4, 0)\n",
    "        # np.clip(labels4[:, 1:] - s / 2, 0, s, out=labels4[:, 1:])  # use with center crop\n",
    "        np.clip(labels4[:, 1:], 0, 2 * s, out=labels4[:, 1:])  # use with random_affine\n",
    "\n",
    "    # Augment\n",
    "    # img4 = img4[s // 2: int(s * 1.5), s // 2:int(s * 1.5)]  # center crop (WARNING, requires box pruning)\n",
    "    img4, labels4 = random_affine(img4, labels4,\n",
    "                                  degrees=self.hyp['degrees'],\n",
    "                                  translate=self.hyp['translate'],\n",
    "                                  scale=self.hyp['scale'],\n",
    "                                  shear=self.hyp['shear'],\n",
    "                                  border=-s // 2)  # border to remove\n",
    "\n",
    "    return img4, labels4\n",
    "\n",
    "\n",
    "def letterbox(img, new_shape=(416, 416), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
    "    # Resize image to a 32-pixel-multiple rectangle https://github.com/ultralytics/yolov3/issues/232\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, 64), np.mod(dh, 64)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = new_shape\n",
    "        ratio = new_shape[0] / shape[1], new_shape[1] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)\n",
    "\n",
    "\n",
    "def random_affine(img, targets=(), degrees=10, translate=.1, scale=.1, shear=10, border=0):\n",
    "    # torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(.1, .1), scale=(.9, 1.1), shear=(-10, 10))\n",
    "    # https://medium.com/uruvideo/dataset-augmentation-with-random-homographies-a8f4b44830d4\n",
    "    # targets = [cls, xyxy]\n",
    "\n",
    "    height = img.shape[0] + border * 2\n",
    "    width = img.shape[1] + border * 2\n",
    "\n",
    "    # Rotation and Scale\n",
    "    R = np.eye(3)\n",
    "    a = random.uniform(-degrees, degrees)\n",
    "    # a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations\n",
    "    s = random.uniform(1 - scale, 1 + scale)\n",
    "    # s = 2 ** random.uniform(-scale, scale)\n",
    "    R[:2] = cv2.getRotationMatrix2D(angle=a, center=(img.shape[1] / 2, img.shape[0] / 2), scale=s)\n",
    "\n",
    "    # Translation\n",
    "    T = np.eye(3)\n",
    "    T[0, 2] = random.uniform(-translate, translate) * img.shape[0] + border  # x translation (pixels)\n",
    "    T[1, 2] = random.uniform(-translate, translate) * img.shape[1] + border  # y translation (pixels)\n",
    "\n",
    "    # Shear\n",
    "    S = np.eye(3)\n",
    "    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # x shear (deg)\n",
    "    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # y shear (deg)\n",
    "\n",
    "    # Combined rotation matrix\n",
    "    M = S @ T @ R  # ORDER IS IMPORTANT HERE!!\n",
    "    if (border != 0) or (M != np.eye(3)).any():  # image changed\n",
    "        img = cv2.warpAffine(img, M[:2], dsize=(width, height), flags=cv2.INTER_LINEAR, borderValue=(114, 114, 114))\n",
    "\n",
    "    # Transform label coordinates\n",
    "    n = len(targets)\n",
    "    if n:\n",
    "        # warp points\n",
    "        xy = np.ones((n * 4, 3))\n",
    "        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n",
    "        xy = (xy @ M.T)[:, :2].reshape(n, 8)\n",
    "\n",
    "        # create new boxes\n",
    "        x = xy[:, [0, 2, 4, 6]]\n",
    "        y = xy[:, [1, 3, 5, 7]]\n",
    "        xy = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n",
    "\n",
    "        # # apply angle-based reduction of bounding boxes\n",
    "        # radians = a * math.pi / 180\n",
    "        # reduction = max(abs(math.sin(radians)), abs(math.cos(radians))) ** 0.5\n",
    "        # x = (xy[:, 2] + xy[:, 0]) / 2\n",
    "        # y = (xy[:, 3] + xy[:, 1]) / 2\n",
    "        # w = (xy[:, 2] - xy[:, 0]) * reduction\n",
    "        # h = (xy[:, 3] - xy[:, 1]) * reduction\n",
    "        # xy = np.concatenate((x - w / 2, y - h / 2, x + w / 2, y + h / 2)).reshape(4, n).T\n",
    "\n",
    "        # reject warped points outside of image\n",
    "        xy[:, [0, 2]] = xy[:, [0, 2]].clip(0, width)\n",
    "        xy[:, [1, 3]] = xy[:, [1, 3]].clip(0, height)\n",
    "        w = xy[:, 2] - xy[:, 0]\n",
    "        h = xy[:, 3] - xy[:, 1]\n",
    "        area = w * h\n",
    "        area0 = (targets[:, 3] - targets[:, 1]) * (targets[:, 4] - targets[:, 2])\n",
    "        ar = np.maximum(w / (h + 1e-16), h / (w + 1e-16))  # aspect ratio\n",
    "        i = (w > 4) & (h > 4) & (area / (area0 * s + 1e-16) > 0.2) & (ar < 10)\n",
    "\n",
    "        targets = targets[i]\n",
    "        targets[:, 1:5] = xy[i]\n",
    "\n",
    "    return img, targets\n",
    "\n",
    "\n",
    "def cutout(image, labels):\n",
    "    # https://arxiv.org/abs/1708.04552\n",
    "    # https://github.com/hysts/pytorch_cutout/blob/master/dataloader.py\n",
    "    # https://towardsdatascience.com/when-conventional-wisdom-fails-revisiting-data-augmentation-for-self-driving-cars-4831998c5509\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    def bbox_ioa(box1, box2):\n",
    "        # Returns the intersection over box2 area given box1, box2. box1 is 4, box2 is nx4. boxes are x1y1x2y2\n",
    "        box2 = box2.transpose()\n",
    "\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n",
    "\n",
    "        # Intersection area\n",
    "        inter_area = (np.minimum(b1_x2, b2_x2) - np.maximum(b1_x1, b2_x1)).clip(0) * \\\n",
    "                     (np.minimum(b1_y2, b2_y2) - np.maximum(b1_y1, b2_y1)).clip(0)\n",
    "\n",
    "        # box2 area\n",
    "        box2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1) + 1e-16\n",
    "\n",
    "        # Intersection over box2 area\n",
    "        return inter_area / box2_area\n",
    "\n",
    "    # create random masks\n",
    "    scales = [0.5] * 1 + [0.25] * 2 + [0.125] * 4 + [0.0625] * 8 + [0.03125] * 16  # image size fraction\n",
    "    for s in scales:\n",
    "        mask_h = random.randint(1, int(h * s))\n",
    "        mask_w = random.randint(1, int(w * s))\n",
    "\n",
    "        # box\n",
    "        xmin = max(0, random.randint(0, w) - mask_w // 2)\n",
    "        ymin = max(0, random.randint(0, h) - mask_h // 2)\n",
    "        xmax = min(w, xmin + mask_w)\n",
    "        ymax = min(h, ymin + mask_h)\n",
    "\n",
    "        # apply random color mask\n",
    "        image[ymin:ymax, xmin:xmax] = [random.randint(64, 191) for _ in range(3)]\n",
    "\n",
    "        # return unobscured labels\n",
    "        if len(labels) and s > 0.03:\n",
    "            box = np.array([xmin, ymin, xmax, ymax], dtype=np.float32)\n",
    "            ioa = bbox_ioa(box, labels[:, 1:5])  # intersection over area\n",
    "            labels = labels[ioa < 0.60]  # remove >60% obscured labels\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def reduce_img_size(path='../data/sm4/images', img_size=1024):  # from utils.datasets import *; reduce_img_size()\n",
    "    # creates a new ./images_reduced folder with reduced size images of maximum size img_size\n",
    "    path_new = path + '_reduced'  # reduced images path\n",
    "    create_folder(path_new)\n",
    "    for f in tqdm(glob.glob('%s/*.*' % path)):\n",
    "        try:\n",
    "            img = cv2.imread(f)\n",
    "            h, w = img.shape[:2]\n",
    "            r = img_size / max(h, w)  # size ratio\n",
    "            if r < 1.0:\n",
    "                img = cv2.resize(img, (int(w * r), int(h * r)), interpolation=cv2.INTER_AREA)  # _LINEAR fastest\n",
    "            fnew = f.replace(path, path_new)  # .replace(Path(f).suffix, '.jpg')\n",
    "            cv2.imwrite(fnew, img)\n",
    "        except:\n",
    "            print('WARNING: image failure %s' % f)\n",
    "\n",
    "\n",
    "def convert_images2bmp():  # from utils.datasets import *; convert_images2bmp()\n",
    "    # Save images\n",
    "    formats = [x.lower() for x in img_formats] + [x.upper() for x in img_formats]\n",
    "    # for path in ['../coco/images/val2014', '../coco/images/train2014']:\n",
    "    for path in ['../data/sm4/images', '../data/sm4/background']:\n",
    "        create_folder(path + 'bmp')\n",
    "        for ext in formats:  # ['.bmp', '.jpg', '.jpeg', '.png', '.tif', '.dng']\n",
    "            for f in tqdm(glob.glob('%s/*%s' % (path, ext)), desc='Converting %s' % ext):\n",
    "                cv2.imwrite(f.replace(ext.lower(), '.bmp').replace(path, path + 'bmp'), cv2.imread(f))\n",
    "\n",
    "    # Save labels\n",
    "    # for path in ['../coco/trainvalno5k.txt', '../coco/5k.txt']:\n",
    "    for file in ['../data/sm4/out_train.txt', '../data/sm4/out_test.txt']:\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.read()\n",
    "            # lines = f.read().replace('2014/', '2014bmp/')  # coco\n",
    "            lines = lines.replace('/images', '/imagesbmp')\n",
    "            lines = lines.replace('/background', '/backgroundbmp')\n",
    "        for ext in formats:\n",
    "            lines = lines.replace(ext, '.bmp')\n",
    "        with open(file.replace('.txt', 'bmp.txt'), 'w') as f:\n",
    "            f.write(lines)\n",
    "\n",
    "\n",
    "def recursive_dataset2bmp(dataset='../data/sm4_bmp'):  # from utils.datasets import *; recursive_dataset2bmp()\n",
    "    # Converts dataset to bmp (for faster training)\n",
    "    formats = [x.lower() for x in img_formats] + [x.upper() for x in img_formats]\n",
    "    for a, b, files in os.walk(dataset):\n",
    "        for file in tqdm(files, desc=a):\n",
    "            p = a + '/' + file\n",
    "            s = Path(file).suffix\n",
    "            if s == '.txt':  # replace text\n",
    "                with open(p, 'r') as f:\n",
    "                    lines = f.read()\n",
    "                for f in formats:\n",
    "                    lines = lines.replace(f, '.bmp')\n",
    "                with open(p, 'w') as f:\n",
    "                    f.write(lines)\n",
    "            elif s in formats:  # replace image\n",
    "                cv2.imwrite(p.replace(s, '.bmp'), cv2.imread(p))\n",
    "                if s != '.bmp':\n",
    "                    os.system(\"rm '%s'\" % p)\n",
    "\n",
    "\n",
    "def imagelist2folder(path='data/coco_64img.txt'):  # from utils.datasets import *; imagelist2folder()\n",
    "    # Copies all the images in a text file (list of images) into a folder\n",
    "    create_folder(path[:-4])\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.read().splitlines():\n",
    "            os.system('cp \"%s\" %s' % (line, path[:-4]))\n",
    "            print(line)\n",
    "\n",
    "\n",
    "def create_folder(path='./new_folder'):\n",
    "    # Create folder\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)  # delete output folder\n",
    "    os.makedirs(path)  # make new output folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMON MODELS #\n",
    "\n",
    "def DWConv(c1, c2, k=1, s=1, act=True):\n",
    "    # Depthwise convolution\n",
    "    return Conv(c1, c2, k, s, g=math.gcd(c1, c2), act=act)\n",
    "\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    # Standard convolution\n",
    "    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):  # ch_in, ch_out, kernel, stride, groups\n",
    "        super(Conv, self).__init__()\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # padding\n",
    "        self.conv = nn.Conv2d(c1, c2, k, s, p, groups=g, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2)\n",
    "        self.act = nn.LeakyReLU(0.1, inplace=True) if act else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def fuseforward(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Standard bottleneck\n",
    "    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\n",
    "        super(Bottleneck, self).__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n",
    "        self.add = shortcut and c1 == c2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
    "\n",
    "\n",
    "class BottleneckCSP(nn.Module):\n",
    "    # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks\n",
    "    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n",
    "        super(BottleneckCSP, self).__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)\n",
    "        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)\n",
    "        self.cv4 = Conv(c2, c2, 1, 1)\n",
    "        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)\n",
    "        self.act = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.cv3(self.m(self.cv1(x)))\n",
    "        y2 = self.cv2(x)\n",
    "        return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))\n",
    "\n",
    "\n",
    "class SPP(nn.Module):\n",
    "    # Spatial pyramid pooling layer used in YOLOv3-SPP\n",
    "    def __init__(self, c1, c2, k=(5, 9, 13)):\n",
    "        super(SPP, self).__init__()\n",
    "        c_ = c1 // 2  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\n",
    "        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x)\n",
    "        return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    # Use after nn.AdaptiveAvgPool2d(1) to remove last 2 dimensions\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class Focus(nn.Module):\n",
    "    # Focus wh information into c-space\n",
    "    def __init__(self, c1, c2, k=1):\n",
    "        super(Focus, self).__init__()\n",
    "        self.conv = Conv(c1 * 4, c2, k, 1)\n",
    "\n",
    "    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)\n",
    "        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))\n",
    "\n",
    "\n",
    "class Concat(nn.Module):\n",
    "    # Concatenate a list of tensors along dimension\n",
    "    def __init__(self, dimension=1):\n",
    "        super(Concat, self).__init__()\n",
    "        self.d = dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat(x, self.d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTAL MODELS #\n",
    "class Sum(nn.Module):\n",
    "    # Weighted sum of 2 or more layers https://arxiv.org/abs/1911.09070\n",
    "    def __init__(self, n, weight=False):  # n: number of inputs\n",
    "        super(Sum, self).__init__()\n",
    "        self.weight = weight  # apply weights boolean\n",
    "        self.iter = range(n - 1)  # iter object\n",
    "        if weight:\n",
    "            self.w = nn.Parameter(-torch.arange(1., n) / 2, requires_grad=True)  # layer weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x[0]  # no weight\n",
    "        if self.weight:\n",
    "            w = torch.sigmoid(self.w) * 2\n",
    "            for i in self.iter:\n",
    "                y = y + x[i + 1] * w[i]\n",
    "        else:\n",
    "            for i in self.iter:\n",
    "                y = y + x[i + 1]\n",
    "        return y\n",
    "\n",
    "\n",
    "class GhostConv(nn.Module):\n",
    "    # Ghost Convolution https://github.com/huawei-noah/ghostnet\n",
    "    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):  # ch_in, ch_out, kernel, stride, groups\n",
    "        super(GhostConv, self).__init__()\n",
    "        c_ = c2 // 2  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, k, s, g, act)\n",
    "        self.cv2 = Conv(c_, c_, 5, 1, c_, act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.cv1(x)\n",
    "        return torch.cat([y, self.cv2(y)], 1)\n",
    "\n",
    "\n",
    "class GhostBottleneck(nn.Module):\n",
    "    # Ghost Bottleneck https://github.com/huawei-noah/ghostnet\n",
    "    def __init__(self, c1, c2, k, s):\n",
    "        super(GhostBottleneck, self).__init__()\n",
    "        c_ = c2 // 2\n",
    "        self.conv = nn.Sequential(GhostConv(c1, c_, 1, 1),  # pw\n",
    "                                  DWConv(c_, c_, k, s, act=False) if s == 2 else nn.Identity(),  # dw\n",
    "                                  GhostConv(c_, c2, 1, 1, act=False))  # pw-linear\n",
    "        self.shortcut = nn.Sequential(DWConv(c1, c1, k, s, act=False),\n",
    "                                      Conv(c1, c2, 1, 1, act=False)) if s == 2 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + self.shortcut(x)\n",
    "\n",
    "\n",
    "class ConvPlus(nn.Module):\n",
    "    # Plus-shaped convolution\n",
    "    def __init__(self, c1, c2, k=3, s=1, g=1, bias=True):  # ch_in, ch_out, kernel, stride, groups\n",
    "        super(ConvPlus, self).__init__()\n",
    "        self.cv1 = nn.Conv2d(c1, c2, (k, 1), s, (k // 2, 0), groups=g, bias=bias)\n",
    "        self.cv2 = nn.Conv2d(c1, c2, (1, k), s, (0, k // 2), groups=g, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cv1(x) + self.cv2(x)\n",
    "\n",
    "\n",
    "class MixConv2d(nn.Module):\n",
    "    # Mixed Depthwise Conv https://arxiv.org/abs/1907.09595\n",
    "    def __init__(self, c1, c2, k=(1, 3), s=1, equal_ch=True):\n",
    "        super(MixConv2d, self).__init__()\n",
    "        groups = len(k)\n",
    "        if equal_ch:  # equal c_ per group\n",
    "            i = torch.linspace(0, groups - 1E-6, c2).floor()  # c2 indices\n",
    "            c_ = [(i == g).sum() for g in range(groups)]  # intermediate channels\n",
    "        else:  # equal weight.numel() per group\n",
    "            b = [c2] + [0] * groups\n",
    "            a = np.eye(groups + 1, groups, k=-1)\n",
    "            a -= np.roll(a, 1, axis=1)\n",
    "            a *= np.array(k) ** 2\n",
    "            a[0] = 1\n",
    "            c_ = np.linalg.lstsq(a, b, rcond=None)[0].round()  # solve for equal weight indices, ax = b\n",
    "\n",
    "        self.m = nn.ModuleList([nn.Conv2d(c1, int(c_[g]), k[g], s, k[g] // 2, bias=False) for g in range(groups)])\n",
    "        self.bn = nn.BatchNorm2d(c2)\n",
    "        self.act = nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.act(self.bn(torch.cat([m(x) for m in self.m], 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO MODEL #\n",
    "class Detect(nn.Module):\n",
    "    def __init__(self, nc=80, anchors=()):  # detection layer\n",
    "        super(Detect, self).__init__()\n",
    "        self.stride = None  # strides computed during build\n",
    "        self.nc = nc  # number of classes\n",
    "        self.no = nc + 5  # number of outputs per anchor\n",
    "        self.nl = len(anchors)  # number of detection layers\n",
    "        self.na = len(anchors[0]) // 2  # number of anchors\n",
    "        self.grid = [torch.zeros(1)] * self.nl  # init grid\n",
    "        a = torch.tensor(anchors).float().view(self.nl, -1, 2)\n",
    "        self.register_buffer('anchors', a)  # shape(nl,na,2)\n",
    "        self.register_buffer('anchor_grid', a.clone().view(self.nl, 1, -1, 1, 1, 2))  # shape(nl,1,na,1,1,2)\n",
    "        self.export = False  # onnx export\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.copy()  # for profiling\n",
    "        z = []  # inference output\n",
    "        self.training |= self.export\n",
    "        for i in range(self.nl):\n",
    "            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n",
    "            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n",
    "\n",
    "            if not self.training:  # inference\n",
    "                if self.grid[i].shape[2:4] != x[i].shape[2:4]:\n",
    "                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)\n",
    "\n",
    "                y = x[i].sigmoid()\n",
    "                y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i].to(x[i].device)) * self.stride[i]  # xy\n",
    "                y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n",
    "                z.append(y.view(bs, -1, self.no))\n",
    "\n",
    "        return x if self.training else (torch.cat(z, 1), x)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_grid(nx=20, ny=20):\n",
    "        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\n",
    "        return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model_cfg='yolov5s.yaml', ch=3, nc=None):  # model, input channels, number of classes\n",
    "        super(Model, self).__init__()\n",
    "        if type(model_cfg) is dict:\n",
    "            self.md = model_cfg  # model dict\n",
    "        else:  # is *.yaml\n",
    "            with open(model_cfg) as f:\n",
    "                self.md = yaml.load(f, Loader=yaml.FullLoader)  # model dict\n",
    "\n",
    "        # Define model\n",
    "        if nc:\n",
    "            self.md['nc'] = nc  # override yaml value\n",
    "        self.model, self.save = parse_model(self.md, ch=[ch])  # model, savelist, ch_out\n",
    "        # print([x.shape for x in self.forward(torch.zeros(1, ch, 64, 64))])\n",
    "\n",
    "        # Build strides, anchors\n",
    "        m = self.model[-1]  # Detect()\n",
    "        m.stride = torch.tensor([128 / x.shape[-2] for x in self.forward(torch.zeros(1, ch, 128, 128))])  # forward\n",
    "        m.anchors /= m.stride.view(-1, 1, 1)\n",
    "        check_anchor_order(m)\n",
    "        self.stride = m.stride\n",
    "\n",
    "        # Init weights, biases\n",
    "        initialize_weights(self)\n",
    "        self._initialize_biases()  # only run once\n",
    "        model_info(self)\n",
    "        print('')\n",
    "\n",
    "    def forward(self, x, augment=False, profile=False):\n",
    "        if augment:\n",
    "            img_size = x.shape[-2:]  # height, width\n",
    "            s = [0.83, 0.67]  # scales\n",
    "            y = []\n",
    "            for i, xi in enumerate((x,\n",
    "                                    scale_img(x.flip(3), s[0]),  # flip-lr and scale\n",
    "                                    scale_img(x, s[1]),  # scale\n",
    "                                    )):\n",
    "                # cv2.imwrite('img%g.jpg' % i, 255 * xi[0].numpy().transpose((1, 2, 0))[:, :, ::-1])\n",
    "                y.append(self.forward_once(xi)[0])\n",
    "\n",
    "            y[1][..., :4] /= s[0]  # scale\n",
    "            y[1][..., 0] = img_size[1] - y[1][..., 0]  # flip lr\n",
    "            y[2][..., :4] /= s[1]  # scale\n",
    "            return torch.cat(y, 1), None  # augmented inference, train\n",
    "        else:\n",
    "            return self.forward_once(x, profile)  # single-scale inference, train\n",
    "\n",
    "    def forward_once(self, x, profile=False):\n",
    "        y, dt = [], []  # outputs\n",
    "        for m in self.model:\n",
    "            if m.f != -1:  # if not from previous layer\n",
    "                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\n",
    "\n",
    "            if profile:\n",
    "                try:\n",
    "                    import thop\n",
    "                    o = thop.profile(m, inputs=(x,), verbose=False)[0] / 1E9 * 2  # FLOPS\n",
    "                except:\n",
    "                    o = 0\n",
    "                t = time_synchronized()\n",
    "                for _ in range(10):\n",
    "                    _ = m(x)\n",
    "                dt.append((time_synchronized() - t) * 100)\n",
    "                print('%10.1f%10.0f%10.1fms %-40s' % (o, m.np, dt[-1], m.type))\n",
    "\n",
    "            x = m(x)  # run\n",
    "            y.append(x if m.i in self.save else None)  # save output\n",
    "\n",
    "        if profile:\n",
    "            print('%.1fms total' % sum(dt))\n",
    "        return x\n",
    "\n",
    "    def _initialize_biases(self, cf=None):  # initialize biases into Detect(), cf is class frequency\n",
    "        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1.\n",
    "        m = self.model[-1]  # Detect() module\n",
    "        for f, s in zip(m.f, m.stride):  # Â from\n",
    "            mi = self.model[f % m.i]\n",
    "            b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)\n",
    "            b[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)\n",
    "            b[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls\n",
    "            mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)\n",
    "\n",
    "    def _print_biases(self):\n",
    "        m = self.model[-1]  # Detect() module\n",
    "        for f in sorted([x % m.i for x in m.f]):  # Â from\n",
    "            b = self.model[f].bias.detach().view(m.na, -1).T  # conv.bias(255) to (3,85)\n",
    "            print(('%g Conv2d.bias:' + '%10.3g' * 6) % (f, *b[:5].mean(1).tolist(), b[5:].mean()))\n",
    "\n",
    "    # def _print_weights(self):\n",
    "    #     for m in self.model.modules():\n",
    "    #         if type(m) is Bottleneck:\n",
    "    #             print('%10.3g' % (m.w.detach().sigmoid() * 2))  # shortcut weights\n",
    "\n",
    "    def fuse(self):  # fuse model Conv2d() + BatchNorm2d() layers\n",
    "        print('Fusing layers...')\n",
    "        for m in self.model.modules():\n",
    "            if type(m) is Conv:\n",
    "                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\n",
    "                m.bn = None  # remove batchnorm\n",
    "                m.forward = m.fuseforward  # update forward\n",
    "        model_info(self)\n",
    "\n",
    "\n",
    "def parse_model(md, ch):  # model_dict, input_channels(3)\n",
    "    print('\\n%3s%15s%3s%10s  %-40s%-30s' % ('', 'from', 'n', 'params', 'module', 'arguments'))\n",
    "    anchors, nc, gd, gw = md['anchors'], md['nc'], md['depth_multiple'], md['width_multiple']\n",
    "    na = (len(anchors[0]) // 2)  # number of anchors\n",
    "    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\n",
    "\n",
    "    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out\n",
    "    for i, (f, n, m, args) in enumerate(md['backbone'] + md['head']):  # from, number, module, args\n",
    "        m = eval(m) if isinstance(m, str) else m  # eval strings\n",
    "        for j, a in enumerate(args):\n",
    "            try:\n",
    "                args[j] = eval(a) if isinstance(a, str) else a  # eval strings\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        n = max(round(n * gd), 1) if n > 1 else n  # depth gain\n",
    "        if m in [nn.Conv2d, Conv, Bottleneck, SPP, DWConv, MixConv2d, Focus, ConvPlus, BottleneckCSP]:\n",
    "            c1, c2 = ch[f], args[0]\n",
    "\n",
    "            # Normal\n",
    "            # if i > 0 and args[0] != no:  # channel expansion factor\n",
    "            #     ex = 1.75  # exponential (default 2.0)\n",
    "            #     e = math.log(c2 / ch[1]) / math.log(2)\n",
    "            #     c2 = int(ch[1] * ex ** e)\n",
    "            # if m != Focus:\n",
    "            c2 = make_divisible(c2 * gw, 8) if c2 != no else c2\n",
    "\n",
    "            # Experimental\n",
    "            # if i > 0 and args[0] != no:  # channel expansion factor\n",
    "            #     ex = 1 + gw  # exponential (default 2.0)\n",
    "            #     ch1 = 32  # ch[1]\n",
    "            #     e = math.log(c2 / ch1) / math.log(2)  # level 1-n\n",
    "            #     c2 = int(ch1 * ex ** e)\n",
    "            # if m != Focus:\n",
    "            #     c2 = make_divisible(c2, 8) if c2 != no else c2\n",
    "\n",
    "            args = [c1, c2, *args[1:]]\n",
    "            if m is BottleneckCSP:\n",
    "                args.insert(2, n)\n",
    "                n = 1\n",
    "        elif m is nn.BatchNorm2d:\n",
    "            args = [ch[f]]\n",
    "        elif m is Concat:\n",
    "            c2 = sum([ch[-1 if x == -1 else x + 1] for x in f])\n",
    "        elif m is Detect:\n",
    "            f = f or list(reversed([(-1 if j == i else j - 1) for j, x in enumerate(ch) if x == no]))\n",
    "        else:\n",
    "            c2 = ch[f]\n",
    "\n",
    "        m_ = nn.Sequential(*[m(*args) for _ in range(n)]) if n > 1 else m(*args)  # module\n",
    "        t = str(m)[8:-2].replace('__main__.', '')  # module type\n",
    "        np = sum([x.numel() for x in m_.parameters()])  # number params\n",
    "        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, 'from' index, type, number params\n",
    "        print('%3s%15s%3s%10.0f  %-40s%-30s' % (i, f, n, np, t, args))  # print\n",
    "        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\n",
    "        layers.append(m_)\n",
    "        ch.append(c2)\n",
    "    return nn.Sequential(*layers), sorted(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST #\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def test(data,\n",
    "         weights=None,\n",
    "         batch_size=16,\n",
    "         imgsz=640,\n",
    "         conf_thres=0.001,\n",
    "         iou_thres=0.6,  # for NMS\n",
    "         save_json=False,\n",
    "         single_cls=False,\n",
    "         augment=False,\n",
    "         verbose=False,\n",
    "         model=None,\n",
    "         dataloader=None,\n",
    "         merge=False):\n",
    "    # Initialize/load model and set device\n",
    "    if model is None:\n",
    "        training = False\n",
    "        device = select_device(opt.device, batch_size=batch_size)\n",
    "        half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "        # Remove previous\n",
    "        for f in glob.glob('test_batch*.jpg'):\n",
    "            os.remove(f)\n",
    "\n",
    "        # Load model\n",
    "        attempt_download(weights)\n",
    "        model = torch.load(weights, map_location=device)['model'].float()  # load to FP32\n",
    "        model_info(model)\n",
    "        model.fuse()\n",
    "        model.to(device)\n",
    "        if half:\n",
    "            model.half()  # to FP16\n",
    "\n",
    "        # Multi-GPU disabled, incompatible with .half()\n",
    "        # if device.type != 'cpu' and torch.cuda.device_count() > 1:\n",
    "        #     model = nn.DataParallel(model)\n",
    "\n",
    "    else:  # called by train.py\n",
    "        training = True\n",
    "        device = next(model.parameters()).device  # get model device\n",
    "        # half disabled https://github.com/ultralytics/yolov5/issues/99\n",
    "        half = False  # device.type != 'cpu' and torch.cuda.device_count() == 1\n",
    "        if half:\n",
    "            model.half()  # to FP16\n",
    "\n",
    "    # Configure\n",
    "    model.eval()\n",
    "    with open(data) as f:\n",
    "        data = yaml.load(f, Loader=yaml.FullLoader)  # model dict\n",
    "    nc = 1 if single_cls else int(data['nc'])  # number of classes\n",
    "    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
    "    # iouv = iouv[0].view(1)  # comment for mAP@0.5:0.95\n",
    "    niou = iouv.numel()\n",
    "\n",
    "    # Dataloader\n",
    "    if dataloader is None:  # not training\n",
    "        img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
    "        _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n",
    "\n",
    "        merge = opt.merge  # use Merge NMS\n",
    "        path = data['test'] if opt.task == 'test' else data['val']  # path to val/test images\n",
    "        dataset = LoadImagesAndLabels(path,\n",
    "                                      imgsz,\n",
    "                                      batch_size,\n",
    "                                      rect=True,  # rectangular inference\n",
    "                                      single_cls=opt.single_cls,  # single class mode\n",
    "                                      stride=int(max(model.stride)),  # model stride\n",
    "                                      pad=0.5)  # padding\n",
    "        batch_size = min(batch_size, len(dataset))\n",
    "        nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "        dataloader = DataLoader(dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                num_workers=nw,\n",
    "                                pin_memory=True,\n",
    "                                collate_fn=dataset.collate_fn)\n",
    "\n",
    "    seen = 0\n",
    "    names = model.names if hasattr(model, 'names') else model.module.names\n",
    "    coco91class = coco80_to_coco91_class()\n",
    "    s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Targets', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n",
    "    p, r, f1, mp, mr, map50, map, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n",
    "    loss = torch.zeros(3, device=device)\n",
    "    jdict, stats, ap, ap_class = [], [], [], []\n",
    "    for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n",
    "        img = img.to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        targets = targets.to(device)\n",
    "        nb, _, height, width = img.shape  # batch size, channels, height, width\n",
    "        whwh = torch.Tensor([width, height, width, height]).to(device)\n",
    "\n",
    "        # Disable gradients\n",
    "        with torch.no_grad():\n",
    "            # Run model\n",
    "            t = time_synchronized()\n",
    "            inf_out, train_out = model(img, augment=augment)  # inference and training outputs\n",
    "            t0 += time_synchronized() - t\n",
    "\n",
    "            # Compute loss\n",
    "            if training:  # if model has loss hyperparameters\n",
    "                loss += compute_loss([x.float() for x in train_out], targets, model)[1][:3]  # GIoU, obj, cls\n",
    "\n",
    "            # Run NMS\n",
    "            t = time_synchronized()\n",
    "            output = non_max_suppression(inf_out, conf_thres=conf_thres, iou_thres=iou_thres, merge=merge)\n",
    "            t1 += time_synchronized() - t\n",
    "\n",
    "        # Statistics per image\n",
    "        for si, pred in enumerate(output):\n",
    "            labels = targets[targets[:, 0] == si, 1:]\n",
    "            nl = len(labels)\n",
    "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "            seen += 1\n",
    "\n",
    "            if pred is None:\n",
    "                if nl:\n",
    "                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
    "                continue\n",
    "\n",
    "            # Append to text file\n",
    "            # with open('test.txt', 'a') as file:\n",
    "            #    [file.write('%11.5g' * 7 % tuple(x) + '\\n') for x in pred]\n",
    "\n",
    "            # Clip boxes to image bounds\n",
    "            clip_coords(pred, (height, width))\n",
    "\n",
    "            # Append to pycocotools JSON dictionary\n",
    "            if save_json:\n",
    "                # [{\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}, ...\n",
    "                image_id = int(Path(paths[si]).stem.split('_')[-1])\n",
    "                box = pred[:, :4].clone()  # xyxy\n",
    "                scale_coords(img[si].shape[1:], box, shapes[si][0], shapes[si][1])  # to original shape\n",
    "                box = xyxy2xywh(box)  # xywh\n",
    "                box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n",
    "                for p, b in zip(pred.tolist(), box.tolist()):\n",
    "                    jdict.append({'image_id': image_id,\n",
    "                                  'category_id': coco91class[int(p[5])],\n",
    "                                  'bbox': [round(x, 3) for x in b],\n",
    "                                  'score': round(p[4], 5)})\n",
    "\n",
    "            # Assign all predictions as incorrect\n",
    "            correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n",
    "            if nl:\n",
    "                detected = []  # target indices\n",
    "                tcls_tensor = labels[:, 0]\n",
    "\n",
    "                # target boxes\n",
    "                tbox = xywh2xyxy(labels[:, 1:5]) * whwh\n",
    "\n",
    "                # Per target class\n",
    "                for cls in torch.unique(tcls_tensor):\n",
    "                    ti = (cls == tcls_tensor).nonzero().view(-1)  # prediction indices\n",
    "                    pi = (cls == pred[:, 5]).nonzero().view(-1)  # target indices\n",
    "\n",
    "                    # Search for detections\n",
    "                    if pi.shape[0]:\n",
    "                        # Prediction to target ious\n",
    "                        ious, i = box_iou(pred[pi, :4], tbox[ti]).max(1)  # best ious, indices\n",
    "\n",
    "                        # Append detections\n",
    "                        for j in (ious > iouv[0]).nonzero():\n",
    "                            d = ti[i[j]]  # detected target\n",
    "                            if d not in detected:\n",
    "                                detected.append(d)\n",
    "                                correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n",
    "                                if len(detected) == nl:  # all targets already located in image\n",
    "                                    break\n",
    "\n",
    "            # Append statistics (correct, conf, pcls, tcls)\n",
    "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
    "\n",
    "        # Plot images\n",
    "        if batch_i < 1:\n",
    "            f = 'test_batch%g_gt.jpg' % batch_i  # filename\n",
    "            plot_images(img, targets, paths, f, names)  # ground truth\n",
    "            f = 'test_batch%g_pred.jpg' % batch_i\n",
    "            plot_images(img, output_to_target(output, width, height), paths, f, names)  # predictions\n",
    "\n",
    "    # Compute statistics\n",
    "    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
    "    if len(stats):\n",
    "        p, r, ap, f1, ap_class = ap_per_class(*stats)\n",
    "        p, r, ap50, ap = p[:, 0], r[:, 0], ap[:, 0], ap.mean(1)  # [P, R, AP@0.5, AP@0.5:0.95]\n",
    "        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
    "    else:\n",
    "        nt = torch.zeros(1)\n",
    "\n",
    "    # Print results\n",
    "    pf = '%20s' + '%12.3g' * 6  # print format\n",
    "    print(pf % ('all', seen, nt.sum(), mp, mr, map50, map))\n",
    "\n",
    "    # Print results per class\n",
    "    if verbose and nc > 1 and len(stats):\n",
    "        for i, c in enumerate(ap_class):\n",
    "            print(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n",
    "\n",
    "    # Print speeds\n",
    "    t = tuple(x / seen * 1E3 for x in (t0, t1, t0 + t1)) + (imgsz, imgsz, batch_size)  # tuple\n",
    "    if not training:\n",
    "        print('Speed: %.1f/%.1f/%.1f ms inference/NMS/total per %gx%g image at batch-size %g' % t)\n",
    "\n",
    "    # Save JSON\n",
    "    if save_json and map50 and len(jdict):\n",
    "        imgIds = [int(Path(x).stem.split('_')[-1]) for x in dataloader.dataset.img_files]\n",
    "        f = 'detections_val2017_%s_results.json' % \\\n",
    "            (weights.split(os.sep)[-1].replace('.pt', '') if weights else '')  # filename\n",
    "        print('\\nCOCO mAP with pycocotools... saving %s...' % f)\n",
    "        with open(f, 'w') as file:\n",
    "            json.dump(jdict, file)\n",
    "\n",
    "        try:\n",
    "            from pycocotools.coco import COCO\n",
    "            from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "            # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\n",
    "            cocoGt = COCO(glob.glob('../coco/annotations/instances_val*.json')[0])  # initialize COCO ground truth api\n",
    "            cocoDt = cocoGt.loadRes(f)  # initialize COCO pred api\n",
    "\n",
    "            cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "            cocoEval.params.imgIds = imgIds  # image IDs to evaluate\n",
    "            cocoEval.evaluate()\n",
    "            cocoEval.accumulate()\n",
    "            cocoEval.summarize()\n",
    "            map, map50 = cocoEval.stats[:2]  # update results (mAP@0.5:0.95, mAP@0.5)\n",
    "        except:\n",
    "            print('WARNING: pycocotools must be installed with numpy==1.17 to run correctly. '\n",
    "                  'See https://github.com/cocodataset/cocoapi/issues/356')\n",
    "\n",
    "    # Return results\n",
    "    maps = np.zeros(nc) + map\n",
    "    for i, c in enumerate(ap_class):\n",
    "        maps[c] = ap[i]\n",
    "    return (mp, mr, map50, map, *(loss.cpu() / len(dataloader)).tolist()), maps, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"\"\"# parameters\n",
    "nc: 1  # number of classes\n",
    "depth_multiple: 1.0  # model depth multiple\n",
    "width_multiple: 1.0  # layer channel multiple\n",
    "\n",
    "# anchors\n",
    "anchors:\n",
    "  - [116,90, 156,198, 373,326]  # P5/32\n",
    "  - [30,61, 62,45, 59,119]  # P4/16\n",
    "  - [10,13, 16,30, 33,23]  # P3/8\n",
    "\n",
    "# YOLOv5 backbone\n",
    "backbone:\n",
    "  # [from, number, module, args]\n",
    "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
    "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
    "   [-1, 3, BottleneckCSP, [128]],\n",
    "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
    "   [-1, 9, BottleneckCSP, [256]],\n",
    "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
    "   [-1, 9, BottleneckCSP, [512]],\n",
    "   [-1, 1, Conv, [1024, 3, 2]], # 7-P5/32\n",
    "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
    "  ]\n",
    "\n",
    "# YOLOv5 head\n",
    "head:\n",
    "  [[-1, 3, BottleneckCSP, [1024, False]],  # 9\n",
    "\n",
    "   [-1, 1, Conv, [512, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
    "   [-1, 3, BottleneckCSP, [512, False]],  # 13\n",
    "\n",
    "   [-1, 1, Conv, [256, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
    "   [-1, 3, BottleneckCSP, [256, False]],\n",
    "   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1]],  # 18 (P3/8-small)\n",
    "\n",
    "   [-2, 1, Conv, [256, 3, 2]],\n",
    "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
    "   [-1, 3, BottleneckCSP, [512, False]],\n",
    "   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1]],  # 22 (P4/16-medium)\n",
    "\n",
    "   [-2, 1, Conv, [512, 3, 2]],\n",
    "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
    "   [-1, 3, BottleneckCSP, [1024, False]],\n",
    "   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1]],  # 26 (P5/32-large)\n",
    "\n",
    "   [[], 1, Detect, [nc, anchors]],  # Detect(P5, P4, P3)\n",
    "  ]\n",
    "\"\"\"\n",
    "with open('yolov5l.yaml', 'w') as f:\n",
    "    f.write(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"\"\"train: ./train/images\n",
    "val: ./valid/images\n",
    "\n",
    "nc: 0\n",
    "names: ['Wheat']\n",
    "\"\"\"\n",
    "with open('data.yaml', 'w') as f:\n",
    "    f.write(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"UNCOMMENT THE FOLLOWING 2 BLOCKS FOR PREPROCESSING\"\"\"\n",
    "\n",
    "# train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# for n ,i in enumerate(glob.glob('./train_images/*.*')[600:]):\n",
    "# #     if n==5:break\n",
    "#     img = cv2.imread(i)\n",
    "#     im = i.split('/')[-1].split('.')[0]\n",
    "#     df = train_df[train_df['image_id']==im]\n",
    "#     labels = []\n",
    "#     for r,j in df.iterrows():\n",
    "#         j.bbox = j.bbox[1:-1].split(',')\n",
    "#         x1, y1, w,h = float(j.bbox[0]),float(j.bbox[1]),float(j.bbox[2]),float(j.bbox[3])\n",
    "#         x,y = x1+w/2, y1+h/2\n",
    "#         x,y,w,h = str(x/1024),str(y/1024),str(w/1024),str(h/1024)\n",
    "#         labels.append([\"1\", x, y ,w, h])\n",
    "\n",
    "#     with open(f'./train/labels/{im}.txt', \"w\",encoding=\"utf-8\") as fo:\n",
    "#         fo.write('\\n'.join([' '.join(i) for i in labels]))\n",
    "        \n",
    "#     cv2.imwrite(f'./train/images/{im}.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for num, k in enumerate(glob.glob('./train/images/*.jpg')):\n",
    "#     if num==2:break\n",
    "#     j = k.replace('jpg','txt').replace('images','labels')\n",
    "#     if os.path.exists(j):\n",
    "#         img = cv2.imread(k)\n",
    "#         good_bbox = []\n",
    "#         with open(j, 'r') as f:\n",
    "#             for i in f:\n",
    "#                 i = i.split(' ')\n",
    "#                 x, y, w, h = float(i[1]),float(i[2]),float(i[3])/2,float(i[4])/2\n",
    "#                 x1, y1 = int(((x+w))*1024),int(((y+h))*1024)\n",
    "#                 x2, y2 = int(((x-w))*1024),int(((y-h))*1024)\n",
    "#                 img = cv2.rectangle(img, (x2,y2), (x1,y1), (0,255,0),4)\n",
    "# #         cv2.imwrite(k.replace('.jpg','_.jpg'),img)\n",
    "#         plt.figure(figsize=(16,9))\n",
    "#         plt.imshow(img)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "opt = AttrDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr0': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'giou': 0.05, 'cls': 0.58, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.014, 'hsv_s': 0.68, 'hsv_v': 0.36, 'degrees': 0.0, 'translate': 0.0, 'scale': 0.5, 'shear': 0.0}\n",
      "{'epochs': 200, 'cfg': './yolov5l.yaml', 'data': './data.yaml', 'img_size': [640], 'rect': False, 'resume': False, 'batch_size': 8, 'nosave': False, 'notest': False, 'noautoanchor': False, 'evolve': False, 'bucket': '', 'cache_images': True, 'weights': '', 'name': 'wheat_detection_0', 'device': '', 'adam': True, 'multi_scale': False, 'single_cls': True}\n",
      "Using CUDA Apex device0 _CudaDeviceProperties(name='GeForce RTX 2080 Ti', total_memory=11016MB)\n",
      "\n",
      "Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/\n",
      "\n",
      "              from  n    params  module                                  arguments                     \n",
      "  0             -1  1      7040  Focus                                   [3, 64, 3]                    \n",
      "  1             -1  1     73984  Conv                                    [64, 128, 3, 2]               \n",
      "  2             -1  1    161152  BottleneckCSP                           [128, 128, 3]                 \n",
      "  3             -1  1    295424  Conv                                    [128, 256, 3, 2]              \n",
      "  4             -1  1   1627904  BottleneckCSP                           [256, 256, 9]                 \n",
      "  5             -1  1   1180672  Conv                                    [256, 512, 3, 2]              \n",
      "  6             -1  1   6499840  BottleneckCSP                           [512, 512, 9]                 \n",
      "  7             -1  1   4720640  Conv                                    [512, 1024, 3, 2]             \n",
      "  8             -1  1   2624512  SPP                                     [1024, 1024, [5, 9, 13]]      \n",
      "  9             -1  1  10234880  BottleneckCSP                           [1024, 1024, 3, False]        \n",
      " 10             -1  1    525312  Conv                                    [1024, 512, 1, 1]             \n",
      " 11             -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12        [-1, 6]  1         0  Concat                                  [1]                           \n",
      " 13             -1  1   2823680  BottleneckCSP                           [1024, 512, 3, False]         \n",
      " 14             -1  1    131584  Conv                                    [512, 256, 1, 1]              \n",
      " 15             -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16        [-1, 4]  1         0  Concat                                  [1]                           \n",
      " 17             -1  1    707328  BottleneckCSP                           [512, 256, 3, False]          \n",
      " 18             -1  1      4626  torch.nn.modules.conv.Conv2d            [256, 18, 1, 1]               \n",
      " 19             -2  1    590336  Conv                                    [256, 256, 3, 2]              \n",
      " 20       [-1, 14]  1         0  Concat                                  [1]                           \n",
      " 21             -1  1   2561536  BottleneckCSP                           [512, 512, 3, False]          \n",
      " 22             -1  1      9234  torch.nn.modules.conv.Conv2d            [512, 18, 1, 1]               \n",
      " 23             -2  1   2360320  Conv                                    [512, 512, 3, 2]              \n",
      " 24       [-1, 10]  1         0  Concat                                  [1]                           \n",
      " 25             -1  1  10234880  BottleneckCSP                           [1024, 1024, 3, False]        \n",
      " 26             -1  1     18450  torch.nn.modules.conv.Conv2d            [1024, 18, 1, 1]              \n",
      " 27   [-1, 22, 18]  1         0  Detect                                  [1, [[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]]\n",
      "Model Summary: 335 layers, 4.73933e+07 parameters, 4.73933e+07 gradients\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching labels train/labels.npy (2386 found, 0 missing, 36 empty, 0 duplicate, for 2422 images): 100%|ââââââââââ| 2422/2422 [00:00<00:00, 17283.47it/s]\n",
      "Caching images:   0%|          | 0/2422 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer groups: 110 .bias, 118 conv.weight, 107 other\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching images (3.0GB): 100%|ââââââââââ| 2422/2422 [00:10<00:00, 235.22it/s]\n",
      "Caching labels valid/labels (987 found, 0 missing, 13 empty, 0 duplicate, for 1000 images): 100%|ââââââââââ| 1000/1000 [00:00<00:00, 7365.39it/s]\n",
      "Caching images (1.2GB): 100%|ââââââââââ| 1000/1000 [00:04<00:00, 235.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing anchors... Best Possible Recall (BPR) = 0.9990\n",
      "Image sizes 640 train, 640 test\n",
      "Using 8 dataloader workers\n",
      "Starting training for 200 epochs...\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/303 [00:00<?, ?it/s]/home/user/anaconda3/envs/py37/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  FutureWarning)\n",
      "     0/199     6.38G   0.07875    0.2956         0    0.3744       311       640: 100%|ââââââââââ| 303/303 [01:25<00:00,  3.53it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95:   0%|          | 0/125 [00:00<?, ?it/s]/home/user/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:157: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [00:55<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.137       0.759       0.287      0.0652\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     1/199      6.4G   0.06997    0.2882         0    0.3582       242       640: 100%|ââââââââââ| 303/303 [01:15<00:00,  3.99it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [00:59<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.136       0.802        0.39       0.122\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     2/199      6.4G   0.06957    0.2865         0     0.356       412       640: 100%|ââââââââââ| 303/303 [01:14<00:00,  4.08it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [00:47<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04      0.0937       0.596       0.193      0.0507\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     3/199      6.4G   0.06995    0.2795         0    0.3495       253       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.11it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.256        0.86       0.703        0.27\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     4/199      6.4G   0.06318    0.2701         0    0.3332       355       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.12it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [00:56<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.398       0.553       0.465       0.165\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     5/199      6.4G   0.06234    0.2712         0    0.3336       550       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:09<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.356       0.869       0.777       0.311\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     6/199      6.4G   0.05893    0.2591         0    0.3181       340       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:17<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.309       0.895       0.766       0.271\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     7/199      6.4G   0.05796    0.2569         0    0.3149       691       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:16<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.412       0.895       0.828       0.342\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     8/199      6.4G   0.05735    0.2588         0    0.3162       431       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.12it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.344       0.898       0.796       0.321\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     9/199      6.4G   0.05668    0.2535         0    0.3102       280       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.12it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:15<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.427       0.888       0.799       0.322\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    10/199      6.4G   0.05639    0.2534         0    0.3098       509       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:09<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.265       0.921       0.797       0.308\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    11/199      6.4G   0.05565    0.2491         0    0.3047       228       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.454       0.883       0.814       0.325\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    12/199      6.4G   0.05486    0.2469         0    0.3018       373       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:16<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.476       0.911       0.871       0.383\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    13/199      6.4G   0.05448    0.2483         0    0.3028       397       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.598       0.839       0.824       0.363\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    14/199      6.4G   0.05437    0.2505         0    0.3048       442       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:14<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04        0.45       0.917       0.874       0.405\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    15/199      6.4G   0.05339    0.2451         0    0.2985       364       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:15<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.458       0.926       0.877         0.4\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    16/199      6.4G   0.05274    0.2442         0     0.297       333       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:17<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.484       0.904        0.86       0.382\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    17/199      6.4G   0.05276    0.2436         0    0.2964       303       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:15<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.508       0.878       0.814       0.344\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    18/199      6.4G   0.05272    0.2426         0    0.2953       297       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:14<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04         0.5       0.922       0.888       0.419\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    19/199      6.4G   0.05193     0.244         0    0.2959       185       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:15<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.506       0.919       0.886       0.416\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    20/199      6.4G   0.05139    0.2404         0    0.2918       318       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:16<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.534       0.899       0.867       0.386\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    21/199      6.4G   0.05141     0.242         0    0.2934       280       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:14<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.581       0.908       0.888       0.411\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    22/199      6.4G   0.05176     0.241         0    0.2928       305       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.498       0.922       0.885       0.428\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    23/199      6.4G   0.05096    0.2375         0    0.2885       285       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.586       0.916       0.894       0.423\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    24/199      6.4G   0.05012    0.2388         0    0.2889       348       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:06<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.332       0.929       0.595       0.259\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    25/199      6.4G   0.05076    0.2376         0    0.2884       224       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.462       0.935       0.896        0.42\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    26/199      6.4G   0.04962    0.2371         0    0.2867       329       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:18<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.646       0.908       0.901       0.443\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    27/199      6.4G   0.05013    0.2375         0    0.2876       329       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:14<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.546       0.932       0.906        0.43\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    28/199      6.4G   0.04957    0.2357         0    0.2852       420       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.12it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:14<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.554       0.936       0.915       0.457\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    29/199      6.4G   0.04921    0.2359         0    0.2852       234       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.611       0.908       0.893       0.434\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    30/199      6.4G   0.05007    0.2371         0    0.2872       267       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [00:39<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04     0.00997      0.0675     0.00202    0.000595\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    31/199      6.4G   0.04928     0.234         0    0.2833       387       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:14<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.564       0.926       0.906       0.441\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    32/199      6.4G     0.049    0.2327         0    0.2817       305       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04        0.52       0.942       0.915       0.446\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    33/199      6.4G   0.04928     0.235         0    0.2842       197       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.424       0.934       0.878       0.385\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    34/199      6.4G   0.04825    0.2325         0    0.2807       418       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.613       0.926       0.912       0.457\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    35/199      6.4G   0.04913    0.2378         0     0.287       145       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:14<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.623       0.924       0.912       0.458\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    36/199      6.4G   0.04809    0.2293         0    0.2774       282       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.581       0.935       0.914       0.463\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    37/199      6.4G   0.04869    0.2333         0     0.282       338       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:16<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.588        0.93       0.914       0.467\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    38/199      6.4G   0.04866    0.2339         0    0.2826       492       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.536       0.936       0.913       0.439\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    39/199      6.4G   0.04874    0.2345         0    0.2832       172       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.574       0.935       0.916       0.465\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    40/199      6.4G   0.04855    0.2343         0    0.2829       469       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.621       0.885        0.87       0.382\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    41/199      6.4G   0.04863    0.2345         0    0.2831       256       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:15<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.617       0.934        0.92       0.471\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    42/199      6.4G   0.04828    0.2331         0    0.2814       236       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.534       0.944       0.921       0.466\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    43/199      6.4G   0.04768    0.2317         0    0.2794       245       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:14<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.615       0.931       0.917       0.469\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    44/199      6.4G   0.04755    0.2321         0    0.2796       386       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.619       0.931       0.919       0.463\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    45/199      6.4G   0.04785    0.2305         0    0.2784       516       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.573        0.94       0.921        0.47\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    46/199      6.4G    0.0474    0.2295         0    0.2769       425       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.503       0.945       0.917       0.463\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    47/199      6.4G   0.04788    0.2323         0    0.2802       348       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:15<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.607       0.924        0.91       0.457\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    48/199      6.4G   0.04769    0.2316         0    0.2793       246       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.593       0.941       0.924       0.483\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    49/199      6.4G   0.04705    0.2305         0    0.2776       284       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:14<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.606       0.936       0.921       0.476\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    50/199      6.4G   0.04738    0.2296         0     0.277       300       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:17<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.658        0.92       0.913       0.468\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    51/199      6.4G   0.04727    0.2312         0    0.2785       284       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.602       0.935        0.92       0.477\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    52/199      6.4G   0.04697    0.2293         0    0.2763       466       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.613       0.938       0.924       0.477\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    53/199      6.4G   0.04667    0.2319         0    0.2786       248       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04        0.62       0.939       0.925       0.482\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    54/199      6.4G   0.04723    0.2329         0    0.2801       238       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.573       0.946       0.927       0.483\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    55/199      6.4G   0.04725    0.2308         0    0.2781       460       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:14<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.609       0.934       0.918        0.47\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    56/199      6.4G   0.04728    0.2279         0    0.2752       227       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.621       0.931        0.92       0.483\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    57/199      6.4G   0.04678    0.2301         0    0.2768       326       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.553       0.946       0.923       0.476\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    58/199      6.4G   0.04682    0.2276         0    0.2744       286       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.612        0.94       0.926       0.486\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    59/199      6.4G    0.0468    0.2297         0    0.2765       403       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:09<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.516       0.949       0.925       0.477\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    60/199      6.4G   0.04709    0.2311         0    0.2782       324       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.615       0.938       0.925       0.482\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    61/199      6.4G     0.047    0.2301         0    0.2771       273       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.628       0.938       0.925       0.486\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    62/199      6.4G   0.04674    0.2275         0    0.2742       187       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.564       0.945       0.925       0.482\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    63/199      6.4G   0.04709    0.2286         0    0.2757       289       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.613       0.933       0.914       0.452\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    64/199      6.4G   0.04666    0.2286         0    0.2753       316       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.628       0.937       0.923       0.481\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    65/199      6.4G     0.047    0.2316         0    0.2786       262       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:14<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.656       0.932       0.924       0.485\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    66/199      6.4G   0.04616    0.2276         0    0.2737       379       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.615        0.94       0.925       0.487\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    67/199      6.4G   0.04621      0.23         0    0.2762       300       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04         0.6       0.939       0.918       0.469\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    68/199      6.4G   0.04632    0.2266         0    0.2729       390       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.592       0.943       0.927       0.492\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    69/199      6.4G   0.04656    0.2289         0    0.2755       537       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.608       0.943       0.929       0.489\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    70/199      6.4G   0.04655    0.2276         0    0.2741       307       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.644       0.938       0.927       0.492\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    71/199      6.4G   0.04616    0.2278         0    0.2739       314       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.15it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.598       0.944       0.929       0.494\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    72/199      6.4G   0.04669    0.2294         0    0.2761       287       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:09<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.547        0.95       0.929       0.489\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    73/199      6.4G   0.04627    0.2258         0     0.272       415       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.623       0.938       0.926       0.491\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    74/199      6.4G   0.04614    0.2279         0     0.274       317       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.644       0.937       0.927       0.493\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    75/199      6.4G   0.04632    0.2293         0    0.2756       348       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04        0.65       0.933       0.925       0.493\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    76/199      6.4G   0.04576    0.2278         0    0.2736       390       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.12it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.659       0.929       0.921       0.479\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    77/199      6.4G   0.04602    0.2303         0    0.2763       426       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04        0.64       0.944       0.933       0.498\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    78/199      6.4G   0.04593    0.2266         0    0.2726       279       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.623       0.943        0.93       0.495\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    79/199      6.4G    0.0459    0.2255         0    0.2714       361       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.607       0.943       0.929       0.495\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    80/199      6.4G   0.04591    0.2285         0    0.2744       389       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.656       0.937       0.928       0.498\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    81/199      6.4G   0.04516    0.2279         0     0.273       318       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.654       0.937       0.928       0.496\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    82/199      6.4G   0.04516    0.2234         0    0.2685       426       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.582       0.948       0.929       0.495\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    83/199      6.4G   0.04553    0.2254         0    0.2709       376       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.642       0.941        0.93         0.5\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    84/199      6.4G   0.04584    0.2254         0    0.2713       413       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04         0.6       0.944       0.929       0.491\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    85/199      6.4G   0.04554    0.2258         0    0.2713       349       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.629       0.944       0.932       0.503\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    86/199      6.4G    0.0458    0.2283         0    0.2741       325       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.633       0.939       0.927       0.498\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    87/199      6.4G   0.04545    0.2268         0    0.2722       411       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.636       0.944       0.933       0.502\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    88/199      6.4G   0.04556    0.2271         0    0.2727       359       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.12it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.626       0.943       0.931       0.501\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    89/199      6.4G    0.0452    0.2251         0    0.2703       345       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.642       0.944       0.932       0.502\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    90/199      6.4G    0.0453    0.2256         0    0.2709       400       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.594       0.947       0.931       0.497\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    91/199      6.4G   0.04597    0.2302         0    0.2762       235       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.641       0.942       0.931       0.502\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    92/199      6.4G   0.04525    0.2256         0    0.2709       459       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.675       0.934       0.928       0.502\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    93/199      6.4G   0.04485    0.2257         0    0.2705       352       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.655        0.94       0.932       0.506\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    94/199      6.4G   0.04535    0.2277         0    0.2731       343       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.662       0.941       0.934       0.503\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    95/199      6.4G   0.04548    0.2282         0    0.2737       299       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.655       0.943       0.932       0.505\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    96/199      6.4G   0.04507    0.2232         0    0.2683       218       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.631       0.945       0.935       0.506\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    97/199      6.4G   0.04467    0.2221         0    0.2668       187       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04        0.63       0.945       0.934       0.503\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    98/199      6.4G   0.04474    0.2239         0    0.2687       521       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.664       0.941       0.932       0.509\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    99/199      6.4G   0.04463    0.2259         0    0.2705       385       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.635       0.945       0.934       0.506\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   100/199      6.4G    0.0446     0.221         0    0.2656       461       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.648       0.944       0.934       0.508\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   101/199      6.4G   0.04481    0.2256         0    0.2704       356       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.645       0.945       0.935       0.506\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   102/199      6.4G   0.04492    0.2246         0    0.2696       297       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.654       0.942       0.931       0.508\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   103/199      6.4G   0.04496    0.2245         0    0.2695       321       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.667       0.941       0.934       0.507\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   104/199      6.4G   0.04456    0.2211         0    0.2657       366       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.632       0.946       0.935       0.505\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   105/199      6.4G   0.04458    0.2231         0    0.2677       261       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04        0.65       0.946       0.937       0.512\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   106/199      6.4G   0.04449    0.2224         0    0.2669       407       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.651       0.945       0.935        0.51\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   107/199      6.4G   0.04452     0.224         0    0.2685       272       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.671       0.942       0.934       0.512\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   108/199      6.4G   0.04469    0.2262         0    0.2709       418       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04        0.64       0.948       0.937        0.51\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   109/199      6.4G   0.04453     0.222         0    0.2665       238       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.666       0.944       0.937       0.513\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   110/199      6.4G   0.04457    0.2231         0    0.2676       482       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.673       0.941       0.935       0.512\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   111/199      6.4G   0.04457    0.2243         0    0.2689       196       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.664       0.943       0.935       0.512\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   112/199      6.4G   0.04447     0.224         0    0.2684       477       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.15it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.634       0.947       0.937       0.513\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   113/199      6.4G   0.04442    0.2243         0    0.2687       195       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.684       0.938       0.933        0.51\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   114/199      6.4G   0.04424    0.2252         0    0.2695       386       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.12it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.637       0.948       0.937       0.509\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   115/199      6.4G   0.04396    0.2238         0    0.2678       167       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.664       0.944       0.937       0.514\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   116/199      6.4G   0.04405    0.2219         0     0.266       445       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.675       0.944       0.937       0.516\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   117/199      6.4G   0.04411    0.2218         0    0.2659       613       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.651       0.948       0.938       0.515\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   118/199      6.4G   0.04393     0.222         0    0.2659       296       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.658       0.947       0.937       0.515\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   119/199      6.4G   0.04415    0.2206         0    0.2647       354       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.655       0.947       0.938       0.514\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   120/199      6.4G   0.04406    0.2207         0    0.2648       322       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.644       0.949       0.939       0.517\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   121/199      6.4G   0.04361    0.2223         0    0.2659       373       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.676       0.944       0.938       0.516\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   122/199      6.4G   0.04395    0.2211         0    0.2651       229       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.664       0.947       0.938       0.517\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   123/199      6.4G   0.04379    0.2213         0    0.2651       697       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04        0.66       0.948        0.94       0.518\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   124/199      6.4G   0.04389    0.2227         0    0.2666       327       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.662       0.946       0.939       0.518\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   125/199      6.4G   0.04358     0.222         0    0.2656       289       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04        0.67       0.943       0.936       0.517\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   126/199      6.4G   0.04367    0.2211         0    0.2647       267       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.661       0.947       0.939       0.518\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   127/199      6.4G   0.04354    0.2228         0    0.2663       323       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.671       0.945       0.938       0.518\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   128/199      6.4G   0.04349    0.2196         0    0.2631       330       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:09<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.651       0.949        0.94       0.519\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   129/199      6.4G   0.04371    0.2217         0    0.2654       403       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:13<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.684       0.942       0.936       0.518\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   130/199      6.4G   0.04383    0.2233         0    0.2671       432       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04        0.66       0.947       0.939       0.518\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   131/199      6.4G   0.04329    0.2223         0    0.2656       371       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.673       0.945       0.939       0.519\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   132/199      6.4G   0.04323    0.2205         0    0.2638       171       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.674       0.947       0.941       0.522\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   133/199      6.4G    0.0432    0.2193         0    0.2626       385       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.663       0.947       0.939       0.522\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   134/199      6.4G   0.04315    0.2184         0    0.2616       335       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04        0.68       0.945       0.939       0.521\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   135/199      6.4G   0.04309    0.2196         0    0.2627       382       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.683       0.944       0.939       0.521\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   136/199      6.4G   0.04341    0.2197         0    0.2631       231       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:09<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.648       0.949        0.94       0.519\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   137/199      6.4G   0.04286    0.2193         0    0.2622       241       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.674       0.946        0.94       0.524\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   138/199      6.4G   0.04313    0.2173         0    0.2604       348       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.691       0.942       0.937        0.52\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   139/199      6.4G   0.04275    0.2186         0    0.2614       503       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:09<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.661       0.949       0.941       0.523\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   140/199      6.4G   0.04276    0.2181         0    0.2609       294       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.669       0.947        0.94       0.523\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   141/199      6.4G   0.04283    0.2184         0    0.2612       451       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.672       0.948        0.94       0.524\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   142/199      6.4G   0.04293     0.218         0     0.261       302       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.684       0.946       0.939       0.523\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   143/199      6.4G   0.04303    0.2192         0    0.2622       355       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.676       0.948        0.94       0.523\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   144/199      6.4G   0.04249    0.2178         0    0.2603       369       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.664       0.947        0.94       0.523\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   145/199      6.4G   0.04261    0.2192         0    0.2618       218       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.687       0.947        0.94       0.523\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   146/199      6.4G     0.042     0.215         0     0.257       294       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.681       0.949       0.941       0.525\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   147/199      6.4G   0.04252    0.2192         0    0.2617       276       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.686       0.945       0.939       0.524\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   148/199      6.4G   0.04257    0.2188         0    0.2613       246       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.672       0.949       0.941       0.526\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   149/199      6.4G   0.04229    0.2166         0    0.2589       500       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.683       0.947       0.941       0.526\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   150/199      6.4G   0.04236    0.2158         0    0.2581       458       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:09<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.667       0.949       0.941       0.525\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   151/199      6.4G   0.04239    0.2174         0    0.2598       266       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.692       0.945        0.94       0.525\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   152/199      6.4G    0.0421    0.2152         0    0.2573       316       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.687       0.947       0.941       0.527\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   153/199      6.4G   0.04231    0.2168         0    0.2592       267       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.683       0.947       0.941       0.526\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   154/199      6.4G   0.04252    0.2174         0    0.2599       279       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.683       0.946       0.941       0.527\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   155/199      6.4G   0.04212    0.2184         0    0.2605       228       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.12it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.684       0.948       0.941       0.526\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   156/199      6.4G   0.04218      0.22         0    0.2622       276       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.685       0.949       0.942       0.527\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   157/199      6.4G   0.04192    0.2162         0    0.2581       250       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.687       0.948       0.942       0.527\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   158/199      6.4G   0.04188    0.2185         0    0.2604       336       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.684       0.947       0.941       0.528\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   159/199      6.4G   0.04192    0.2149         0    0.2568       358       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.671        0.95       0.943       0.527\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   160/199      6.4G   0.04181     0.219         0    0.2608       433       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.689       0.948       0.942       0.529\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   161/199      6.4G   0.04173    0.2167         0    0.2584       340       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.682       0.948       0.941       0.529\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   162/199      6.4G   0.04176    0.2163         0    0.2581       365       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.684       0.948       0.942       0.529\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   163/199      6.4G   0.04195    0.2161         0    0.2581       301       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.695       0.945        0.94       0.529\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   164/199      6.4G   0.04139    0.2129         0    0.2543       514       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.689       0.946       0.941       0.529\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   165/199      6.4G   0.04132    0.2141         0    0.2554       414       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04        0.69       0.947       0.941        0.53\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   166/199      6.4G    0.0413    0.2166         0    0.2579       237       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04        0.69       0.948       0.943        0.53\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   167/199      6.4G   0.04144    0.2139         0    0.2553       439       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.698       0.945       0.941       0.529\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   168/199      6.4G   0.04152    0.2169         0    0.2584       285       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.693       0.947       0.942        0.53\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   169/199      6.4G   0.04104     0.214         0     0.255       341       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.693       0.947       0.942       0.529\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   170/199      6.4G   0.04142     0.217         0    0.2584       367       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.693       0.947       0.942        0.53\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   171/199      6.4G   0.04127    0.2159         0    0.2572       382       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.694       0.947       0.942       0.531\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   172/199      6.4G   0.04117    0.2129         0    0.2541       372       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.697       0.947       0.942       0.531\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   173/199      6.4G   0.04132    0.2151         0    0.2565       270       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04        0.69       0.949       0.943       0.531\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   174/199      6.4G   0.04122    0.2143         0    0.2556       219       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.694       0.947       0.942       0.531\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   175/199      6.4G   0.04127    0.2144         0    0.2556       594       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.695       0.947       0.943       0.531\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   176/199      6.4G   0.04089    0.2119         0    0.2528       165       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.689       0.949       0.943       0.531\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   177/199      6.4G   0.04103    0.2129         0    0.2539       555       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.691       0.948       0.942       0.532\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   178/199      6.4G   0.04122    0.2145         0    0.2557       386       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.699       0.948       0.943       0.532\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   179/199      6.4G   0.04094    0.2129         0    0.2538       485       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.697       0.946       0.942       0.532\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   180/199      6.4G   0.04076    0.2129         0    0.2537       194       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.699       0.947       0.943       0.533\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   181/199      6.4G   0.04077    0.2112         0     0.252       398       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.695       0.949       0.943       0.533\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   182/199      6.4G   0.04072    0.2116         0    0.2523       332       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.696       0.948       0.942       0.533\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   183/199      6.4G   0.04076    0.2118         0    0.2525       330       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.15it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.696       0.948       0.943       0.533\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   184/199      6.4G   0.04078    0.2115         0    0.2522       438       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.698       0.948       0.943       0.533\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   185/199      6.4G   0.04064    0.2137         0    0.2544       327       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04         0.7       0.947       0.942       0.532\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   186/199      6.4G   0.04055    0.2114         0    0.2519       243       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.698       0.946       0.942       0.532\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   187/199      6.4G   0.04047    0.2107         0    0.2512       293       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.699       0.947       0.943       0.533\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   188/199      6.4G   0.04056    0.2113         0    0.2519       380       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.698       0.948       0.943       0.533\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   189/199      6.4G   0.04059    0.2115         0    0.2521       299       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.695       0.948       0.943       0.533\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   190/199      6.4G   0.04063     0.211         0    0.2516       133       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.702       0.947       0.943       0.533\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   191/199      6.4G   0.04074    0.2109         0    0.2516       360       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.695       0.949       0.943       0.534\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   192/199      6.4G   0.04064    0.2114         0     0.252       310       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.695       0.948       0.942       0.533\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   193/199      6.4G    0.0406    0.2134         0     0.254       285       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.698       0.948       0.943       0.533\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   194/199      6.4G   0.04047    0.2125         0     0.253       403       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.703       0.947       0.942       0.533\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   195/199      6.4G   0.04043    0.2099         0    0.2503       362       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:11<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.704       0.947       0.942       0.532\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   196/199      6.4G   0.04061    0.2138         0    0.2544       230       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04         0.7       0.948       0.943       0.534\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   197/199      6.4G    0.0404    0.2112         0    0.2516       470       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.13it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.698       0.947       0.942       0.534\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   198/199      6.4G   0.04015      0.21         0    0.2502       249       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.704       0.948       0.943       0.535\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   199/199      6.4G   0.04043    0.2119         0    0.2523       440       640: 100%|ââââââââââ| 303/303 [01:13<00:00,  4.14it/s]\n",
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|ââââââââââ| 125/125 [01:10<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       1e+03    4.36e+04       0.699       0.948       0.943       0.534\n",
      "Optimizer stripped from weights/last_wheat_detection_0.pt\n",
      "Optimizer stripped from weights/best_wheat_detection_0.pt\n",
      "200 epochs completed in 8.137 hours.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TRAIN ##\n",
    "import argparse\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "mixed_precision = True\n",
    "try:  # Mixed precision training https://github.com/NVIDIA/apex\n",
    "    from apex import amp\n",
    "except:\n",
    "    print('Apex recommended for faster mixed precision training: https://github.com/NVIDIA/apex')\n",
    "    mixed_precision = False  # not installed\n",
    "\n",
    "wdir = 'weights' + os.sep  # weights dir\n",
    "os.makedirs(wdir, exist_ok=True)\n",
    "last = wdir + 'last.pt'\n",
    "best = wdir + 'best.pt'\n",
    "results_file = 'results.txt'\n",
    "\n",
    "# Hyperparameters\n",
    "hyp = {'lr0': 0.01,  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "       'momentum': 0.937,  # SGD momentum\n",
    "       'weight_decay': 5e-4,  # optimizer weight decay\n",
    "       'giou': 0.05,  # giou loss gain\n",
    "       'cls': 0.58,  # cls loss gain\n",
    "       'cls_pw': 1.0,  # cls BCELoss positive_weight\n",
    "       'obj': 1.0,  # obj loss gain (*=img_size/320 if img_size != 320)\n",
    "       'obj_pw': 1.0,  # obj BCELoss positive_weight\n",
    "       'iou_t': 0.20,  # iou training threshold\n",
    "       'anchor_t': 4.0,  # anchor-multiple threshold\n",
    "       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n",
    "       'hsv_h': 0.014,  # image HSV-Hue augmentation (fraction)\n",
    "       'hsv_s': 0.68,  # image HSV-Saturation augmentation (fraction)\n",
    "       'hsv_v': 0.36,  # image HSV-Value augmentation (fraction)\n",
    "       'degrees': 0.0,  # image rotation (+/- deg)\n",
    "       'translate': 0.0,  # image translation (+/- fraction)\n",
    "       'scale': 0.5,  # image scale (+/- gain)\n",
    "       'shear': 0.0}  # image shear (+/- deg)\n",
    "print(hyp)\n",
    "\n",
    "# Overwrite hyp with hyp*.txt (optional)\n",
    "f = glob.glob('hyp*.txt')\n",
    "if f:\n",
    "    print('Using %s' % f[0])\n",
    "    for k, v in zip(hyp.keys(), np.loadtxt(f[0])):\n",
    "        hyp[k] = v\n",
    "\n",
    "# Print focal loss if gamma > 0\n",
    "if hyp['fl_gamma']:\n",
    "    print('Using FocalLoss(gamma=%g)' % hyp['fl_gamma'])\n",
    "\n",
    "\n",
    "def train(hyp):\n",
    "    epochs = opt.epochs  # 300\n",
    "    batch_size = opt.batch_size  # 64\n",
    "    weights = opt.weights  # initial training weights\n",
    "\n",
    "    # Configure\n",
    "    init_seeds(1)\n",
    "    with open(opt.data) as f:\n",
    "        data_dict = yaml.load(f, Loader=yaml.FullLoader)  # model dict\n",
    "    train_path = data_dict['train']\n",
    "    test_path = data_dict['val']\n",
    "    nc = 1 if opt.single_cls else int(data_dict['nc'])  # number of classes\n",
    "\n",
    "    # Remove previous results\n",
    "    for f in glob.glob('*_batch*.jpg') + glob.glob(results_file):\n",
    "        os.remove(f)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(opt.cfg).to(device)\n",
    "    assert model.md['nc'] == nc, '%s nc=%g classes but %s nc=%g classes' % (opt.data, nc, opt.cfg, model.md['nc'])\n",
    "\n",
    "    # Image sizes\n",
    "    gs = int(max(model.stride))  # grid size (max stride)\n",
    "    imgsz, imgsz_test = [check_img_size(x, gs) for x in opt.img_size]  # verify imgsz are gs-multiples\n",
    "\n",
    "    # Optimizer\n",
    "    nbs = 64  # nominal batch size\n",
    "    accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing\n",
    "    hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay\n",
    "    pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n",
    "    for k, v in model.named_parameters():\n",
    "        if v.requires_grad:\n",
    "            if '.bias' in k:\n",
    "                pg2.append(v)  # biases\n",
    "            elif '.weight' in k and '.bn' not in k:\n",
    "                pg1.append(v)  # apply weight decay\n",
    "            else:\n",
    "                pg0.append(v)  # all else\n",
    "\n",
    "    optimizer = optim.Adam(pg0, lr=hyp['lr0']) if opt.adam else \\\n",
    "        optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n",
    "    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n",
    "    optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n",
    "    print('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))\n",
    "    del pg0, pg1, pg2\n",
    "\n",
    "    # Load Model\n",
    "    attempt_download(weights)\n",
    "    start_epoch, best_fitness = 0, 0.0\n",
    "    if weights.endswith('.pt'):  # pytorch format\n",
    "        ckpt = torch.load(weights, map_location=device)  # load checkpoint\n",
    "\n",
    "        # load model\n",
    "        try:\n",
    "            ckpt['model'] = {k: v for k, v in ckpt['model'].float().state_dict().items()\n",
    "                             if model.state_dict()[k].shape == v.shape}  # to FP32, filter\n",
    "            model.load_state_dict(ckpt['model'], strict=False)\n",
    "        except KeyError as e:\n",
    "            s = \"%s is not compatible with %s. Specify --weights '' or specify a --cfg compatible with %s.\" \\\n",
    "                % (opt.weights, opt.cfg, opt.weights)\n",
    "            raise KeyError(s) from e\n",
    "\n",
    "        # load optimizer\n",
    "        if ckpt['optimizer'] is not None:\n",
    "            optimizer.load_state_dict(ckpt['optimizer'])\n",
    "            best_fitness = ckpt['best_fitness']\n",
    "\n",
    "        # load results\n",
    "        if ckpt.get('training_results') is not None:\n",
    "            with open(results_file, 'w') as file:\n",
    "                file.write(ckpt['training_results'])  # write results.txt\n",
    "\n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        del ckpt\n",
    "\n",
    "    # Mixed precision training https://github.com/NVIDIA/apex\n",
    "    if mixed_precision:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n",
    "\n",
    "    # Scheduler https://arxiv.org/pdf/1812.01187.pdf\n",
    "    lf = lambda x: (((1 + math.cos(x * math.pi / epochs)) / 2) ** 1.0) * 0.9 + 0.1  # cosine\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n",
    "    scheduler.last_epoch = start_epoch - 1  # do not move\n",
    "    # https://discuss.pytorch.org/t/a-problem-occured-when-resuming-an-optimizer/28822\n",
    "    # plot_lr_scheduler(optimizer, scheduler, epochs)\n",
    "\n",
    "    # Initialize distributed training\n",
    "    if device.type != 'cpu' and torch.cuda.device_count() > 1 and torch.distributed.is_available():\n",
    "        dist.init_process_group(backend='nccl',  # distributed backend\n",
    "                                init_method='tcp://127.0.0.1:9999',  # init method\n",
    "                                world_size=1,  # number of nodes\n",
    "                                rank=0)  # node rank\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "        # pip install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "    # Dataset\n",
    "    dataset = LoadImagesAndLabels(train_path, imgsz, batch_size,\n",
    "                                  augment=True,\n",
    "                                  hyp=hyp,  # augmentation hyperparameters\n",
    "                                  rect=opt.rect,  # rectangular training\n",
    "                                  cache_images=opt.cache_images,\n",
    "                                  single_cls=opt.single_cls,\n",
    "                                  stride=gs)\n",
    "    mlc = np.concatenate(dataset.labels, 0)[:, 0].max()  # max label class\n",
    "    assert mlc <= nc, 'Label class %g exceeds nc=%g in %s. Correct your labels or your model.' % (mlc, nc, opt.cfg)\n",
    "\n",
    "    # Dataloader\n",
    "    batch_size = min(batch_size, len(dataset))\n",
    "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             num_workers=nw,\n",
    "                                             shuffle=not opt.rect,  # Shuffle=True unless rectangular training is used\n",
    "                                             pin_memory=True,\n",
    "                                             collate_fn=dataset.collate_fn)\n",
    "\n",
    "    # Testloader\n",
    "    testloader = torch.utils.data.DataLoader(LoadImagesAndLabels(test_path, imgsz_test, batch_size,\n",
    "                                                                 hyp=hyp,\n",
    "                                                                 rect=True,\n",
    "                                                                 cache_images=opt.cache_images,\n",
    "                                                                 single_cls=opt.single_cls,\n",
    "                                                                 stride=gs),\n",
    "                                             batch_size=batch_size,\n",
    "                                             num_workers=nw,\n",
    "                                             pin_memory=True,\n",
    "                                             collate_fn=dataset.collate_fn)\n",
    "\n",
    "    hyp['cls'] *= nc / 80.  # scale coco-tuned hyp['cls'] to current dataset\n",
    "    model.nc = nc  # attach number of classes to model\n",
    "    model.hyp = hyp  # attach hyperparameters to model\n",
    "    model.gr = 1.0  # giou loss ratio (obj_loss = 1.0 or giou)\n",
    "    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device)  # attach class weights\n",
    "    model.names = data_dict['names']\n",
    "\n",
    "    labels = np.concatenate(dataset.labels, 0)\n",
    "    c = torch.tensor(labels[:, 0])  # classes\n",
    "    if tb_writer:\n",
    "        plot_labels(labels)\n",
    "        tb_writer.add_histogram('classes', c, 0)\n",
    "\n",
    "    if not opt.noautoanchor:\n",
    "        check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\n",
    "\n",
    "    ema = ModelEMA(model)\n",
    "\n",
    "    t0 = time.time()\n",
    "    nb = len(dataloader)  # number of batches\n",
    "    n_burn = max(3 * nb, 1e3)  # burn-in iterations, max(3 epochs, 1k iterations)\n",
    "    maps = np.zeros(nc)  # mAP per class\n",
    "    results = (0, 0, 0, 0, 0, 0, 0)  # 'P', 'R', 'mAP', 'F1', 'val GIoU', 'val Objectness', 'val Classification'\n",
    "    print('Image sizes %g train, %g test' % (imgsz, imgsz_test))\n",
    "    print('Using %g dataloader workers' % nw)\n",
    "    print('Starting training for %g epochs...' % epochs)\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n",
    "        model.train()\n",
    "\n",
    "        # Update image weights (optional)\n",
    "        if dataset.image_weights:\n",
    "            w = model.class_weights.cpu().numpy() * (1 - maps) ** 2  # class weights\n",
    "            image_weights = labels_to_image_weights(dataset.labels, nc=nc, class_weights=w)\n",
    "            dataset.indices = random.choices(range(dataset.n), weights=image_weights, k=dataset.n)  # rand weighted idx\n",
    "\n",
    "        mloss = torch.zeros(4, device=device)  # mean losses\n",
    "        print(('\\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'GIoU', 'obj', 'cls', 'total', 'targets', 'img_size'))\n",
    "        pbar = tqdm(enumerate(dataloader), total=nb)  # progress bar\n",
    "        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
    "            ni = i + nb * epoch  # number integrated batches (since train start)\n",
    "            imgs = imgs.to(device).float() / 255.0  # uint8 to float32, 0 - 255 to 0.0 - 1.0\n",
    "\n",
    "            # Burn-in\n",
    "            if ni <= n_burn:\n",
    "                xi = [0, n_burn]  # x interp\n",
    "                # model.gr = np.interp(ni, xi, [0.0, 1.0])  # giou loss ratio (obj_loss = 1.0 or giou)\n",
    "                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\n",
    "                for j, x in enumerate(optimizer.param_groups):\n",
    "                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
    "                    x['lr'] = np.interp(ni, xi, [0.1 if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
    "                    if 'momentum' in x:\n",
    "                        x['momentum'] = np.interp(ni, xi, [0.9, hyp['momentum']])\n",
    "\n",
    "            if opt.multi_scale:\n",
    "                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n",
    "                sf = sz / max(imgs.shape[2:])  # scale factor\n",
    "                if sf != 1:\n",
    "                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n",
    "                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n",
    "\n",
    "            pred = model(imgs)\n",
    "            loss, loss_items = compute_loss(pred, targets.to(device), model)\n",
    "            if not torch.isfinite(loss):\n",
    "                print('WARNING: non-finite loss, ending training ', loss_items)\n",
    "                return results\n",
    "            # Backward\n",
    "            if mixed_precision:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if ni % accumulate == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                ema.update(model)\n",
    "            mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n",
    "            mem = '%.3gG' % (torch.cuda.memory_cached() / 1E9 if torch.cuda.is_available() else 0)  # (GB)\n",
    "            s = ('%10s' * 2 + '%10.4g' * 6) % (\n",
    "                '%g/%g' % (epoch, epochs - 1), mem, *mloss, targets.shape[0], imgs.shape[-1])\n",
    "            pbar.set_description(s)\n",
    "            if ni < 3:\n",
    "                f = 'train_batch%g.jpg' % i  # filename\n",
    "                res = plot_images(images=imgs, targets=targets, paths=paths, fname=f)\n",
    "                if tb_writer:\n",
    "                    tb_writer.add_image(f, res, dataformats='HWC', global_step=epoch)\n",
    "        scheduler.step()\n",
    "        # mAP\n",
    "        ema.update_attr(model)\n",
    "        final_epoch = epoch + 1 == epochs\n",
    "        if not opt.notest or final_epoch:  # Calculate mAP\n",
    "            results, maps, times = test(opt.data,\n",
    "                                             batch_size=batch_size,\n",
    "                                             imgsz=imgsz_test,\n",
    "                                             save_json=final_epoch and opt.data.endswith(os.sep + 'coco.yaml'),\n",
    "                                             model=ema.ema,\n",
    "                                             single_cls=opt.single_cls,\n",
    "                                             dataloader=testloader)\n",
    "\n",
    "        # Write\n",
    "        with open(results_file, 'a') as f:\n",
    "            f.write(s + '%10.4g' * 7 % results + '\\n')  # P, R, mAP, F1, test_losses=(GIoU, obj, cls)\n",
    "        if len(opt.name) and opt.bucket:\n",
    "            os.system('gsutil cp results.txt gs://%s/results/results%s.txt' % (opt.bucket, opt.name))\n",
    "\n",
    "        # Tensorboard\n",
    "        if tb_writer:\n",
    "            tags = ['train/giou_loss', 'train/obj_loss', 'train/cls_loss',\n",
    "                    'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/F1',\n",
    "                    'val/giou_loss', 'val/obj_loss', 'val/cls_loss']\n",
    "            for x, tag in zip(list(mloss[:-1]) + list(results), tags):\n",
    "                tb_writer.add_scalar(tag, x, epoch)\n",
    "\n",
    "        # Update best mAP\n",
    "        fi = fitness(np.array(results).reshape(1, -1))  # fitness_i = weighted combination of [P, R, mAP, F1]\n",
    "        if fi > best_fitness:\n",
    "            best_fitness = fi\n",
    "\n",
    "        # Save model\n",
    "        save = (not opt.nosave) or (final_epoch and not opt.evolve)\n",
    "        if save:\n",
    "            with open(results_file, 'r') as f:  # create checkpoint\n",
    "                ckpt = {'epoch': epoch,\n",
    "                        'best_fitness': best_fitness,\n",
    "                        'training_results': f.read(),\n",
    "                        'model': ema.ema.module if hasattr(model, 'module') else ema.ema,\n",
    "                        'optimizer': None if final_epoch else optimizer.state_dict()}\n",
    "\n",
    "            # Save last, best and delete\n",
    "            torch.save(ckpt, last)\n",
    "            if (best_fitness == fi) and not final_epoch:\n",
    "                torch.save(ckpt, best)\n",
    "            del ckpt\n",
    "\n",
    "        # end epoch ----------------------------------------------------------------------------------------------------\n",
    "    # end training\n",
    "\n",
    "    n = opt.name\n",
    "    if len(n):\n",
    "        n = '_' + n if not n.isnumeric() else n\n",
    "        fresults, flast, fbest = 'results%s.txt' % n, wdir + 'last%s.pt' % n, wdir + 'best%s.pt' % n\n",
    "        for f1, f2 in zip([wdir + 'last.pt', wdir + 'best.pt', 'results.txt'], [flast, fbest, fresults]):\n",
    "            if os.path.exists(f1):\n",
    "                os.rename(f1, f2)  # rename\n",
    "                ispt = f2.endswith('.pt')  # is *.pt\n",
    "                strip_optimizer(f2) if ispt else None  # strip optimizer\n",
    "                os.system('gsutil cp %s gs://%s/weights' % (f2, opt.bucket)) if opt.bucket and ispt else None  # upload\n",
    "\n",
    "    if not opt.evolve:\n",
    "        plot_results()  # save as results.png\n",
    "    print('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))\n",
    "    dist.destroy_process_group() if device.type != 'cpu' and torch.cuda.device_count() > 1 else None\n",
    "    torch.cuda.empty_cache()\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--epochs', type=int, default=300)\n",
    "#     parser.add_argument('--batch-size', type=int, default=16)\n",
    "#     parser.add_argument('--cfg', type=str, default='models/yolov5s.yaml', help='*.cfg path')\n",
    "#     parser.add_argument('--data', type=str, default='data/coco128.yaml', help='*.data path')\n",
    "#     parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='train,test sizes')\n",
    "#     parser.add_argument('--rect', action='store_true', help='rectangular training')\n",
    "#     parser.add_argument('--resume', action='store_true', help='resume training from last.pt')\n",
    "#     parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')\n",
    "#     parser.add_argument('--notest', action='store_true', help='only test final epoch')\n",
    "#     parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')\n",
    "#     parser.add_argument('--evolve', action='store_true', help='evolve hyperparameters')\n",
    "#     parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')\n",
    "#     parser.add_argument('--cache-images', action='store_true', help='cache images for faster training')\n",
    "#     parser.add_argument('--weights', type=str, default='', help='initial weights path')\n",
    "#     parser.add_argument('--name', default='', help='renames results.txt to results_name.txt if supplied')\n",
    "#     parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "#     parser.add_argument('--adam', action='store_true', help='use adam optimizer')\n",
    "#     parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%')\n",
    "#     parser.add_argument('--single-cls', action='store_true', help='train as single-class dataset')\n",
    "#     opt = parser.parse_args()\n",
    "    \n",
    "    \n",
    "#     opt.weights = last if opt.resume else opt.weights\n",
    "    opt.update({\n",
    "        'epochs': 200,'cfg':'./yolov5l.yaml','data':'./data.yaml','img_size':[640],'rect':False,'resume':False,\n",
    "        'batch_size': 8,'nosave':False,'notest':False,'noautoanchor':False,'evolve':False,'bucket':'',\n",
    "        'cache_images':True,'weights':'','name':'wheat_detection_0','device':'','adam':True,'multi_scale':False,'single_cls':True\n",
    "    })\n",
    "    opt.cfg = check_file(opt.cfg)  # check file\n",
    "    opt.data = check_file(opt.data)  # check file\n",
    "    print(opt)\n",
    "    opt.img_size.extend([opt.img_size[-1]] * (2 - len(opt.img_size)))  # extend to 2 sizes (train, test)\n",
    "    device = select_device(opt.device, apex=mixed_precision, batch_size=opt.batch_size)\n",
    "    if device.type == 'cpu':\n",
    "        mixed_precision = False\n",
    "\n",
    "    # Train\n",
    "    if not opt.evolve:\n",
    "        tb_writer = SummaryWriter(comment=opt.name)\n",
    "        print('Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/')\n",
    "        train(hyp)\n",
    "\n",
    "    # Evolve hyperparameters (optional)\n",
    "    else:\n",
    "        tb_writer = None\n",
    "        opt.notest, opt.nosave = True, True  # only test/save final epoch\n",
    "        if opt.bucket:\n",
    "            os.system('gsutil cp gs://%s/evolve.txt .' % opt.bucket)  # download evolve.txt if exists\n",
    "\n",
    "        for _ in range(10):  # generations to evolve\n",
    "            if os.path.exists('evolve.txt'):  # if evolve.txt exists: select best hyps and mutate\n",
    "                # Select parent(s)\n",
    "                parent = 'single'  # parent selection method: 'single' or 'weighted'\n",
    "                x = np.loadtxt('evolve.txt', ndmin=2)\n",
    "                n = min(5, len(x))  # number of previous results to consider\n",
    "                x = x[np.argsort(-fitness(x))][:n]  # top n mutations\n",
    "                w = fitness(x) - fitness(x).min()  # weights\n",
    "                if parent == 'single' or len(x) == 1:\n",
    "                    # x = x[random.randint(0, n - 1)]  # random selection\n",
    "                    x = x[random.choices(range(n), weights=w)[0]]  # weighted selection\n",
    "                elif parent == 'weighted':\n",
    "                    x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination\n",
    "\n",
    "                # Mutate\n",
    "                mp, s = 0.9, 0.2  # mutation probability, sigma\n",
    "                npr = np.random\n",
    "                npr.seed(int(time.time()))\n",
    "                g = np.array([1, 1, 1, 1, 1, 1, 1, 0, .1, 1, 0, 1, 1, 1, 1, 1, 1, 1])  # gains\n",
    "                ng = len(g)\n",
    "                v = np.ones(ng)\n",
    "                while all(v == 1):  # mutate until a change occurs (prevent duplicates)\n",
    "                    v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)\n",
    "                for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)\n",
    "                    hyp[k] = x[i + 7] * v[i]  # mutate\n",
    "\n",
    "            # Clip to limits\n",
    "            keys = ['lr0', 'iou_t', 'momentum', 'weight_decay', 'hsv_s', 'hsv_v', 'translate', 'scale', 'fl_gamma']\n",
    "            limits = [(1e-5, 1e-2), (0.00, 0.70), (0.60, 0.98), (0, 0.001), (0, .9), (0, .9), (0, .9), (0, .9), (0, 3)]\n",
    "            for k, v in zip(keys, limits):\n",
    "                hyp[k] = np.clip(hyp[k], v[0], v[1])\n",
    "\n",
    "            # Train mutation\n",
    "            results = train(hyp.copy())\n",
    "\n",
    "            # Write mutation results\n",
    "            print_mutation(hyp, results, opt.bucket)\n",
    "\n",
    "            # Plot results\n",
    "            # plot_evolution_results(hyp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_wheat_detection_0.pt', 'last_wheat_detection_0.pt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "opt = AttrDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': './weights/best_wheat_detection_0.pt', 'source': './test', 'img_size': 640, 'output': 'inference/output', 'save_txt': True, 'conf-thres': 0.2, 'iou-thres': 0.6, 'fourcc': 'mp4v', 'device': '', 'view_img': False, 'classes': [0], 'agnostic_nms': False, 'augment': False, 'conf_thres': 0.0, 'iou_thres': 0.6}\n",
      "Using CUDA device0 _CudaDeviceProperties(name='GeForce RTX 2080 Ti', total_memory=11016MB)\n",
      "\n",
      "image 1/10 test/2fd875eaa.jpg: 640x640 300 Wheats, Done. (0.047s)\n",
      "image 2/10 test/348a992bb.jpg: 640x640 300 Wheats, Done. (0.049s)\n",
      "image 3/10 test/51b3e36ab.jpg: 640x640 300 Wheats, Done. (0.046s)\n",
      "image 4/10 test/51f1be19e.jpg: 640x640 300 Wheats, Done. (0.046s)\n",
      "image 5/10 test/53f253011.jpg: "
     ]
    }
   ],
   "source": [
    "\n",
    "def detect(save_img=False):\n",
    "    out, source, weights, view_img, save_txt, imgsz = \\\n",
    "        opt.output, opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size\n",
    "    webcam = source == '0' or source.startswith('rtsp') or source.startswith('http') or source.endswith('.txt')\n",
    "\n",
    "    # Initialize\n",
    "    device = select_device(opt.device)\n",
    "    if os.path.exists(out):\n",
    "        shutil.rmtree(out)  # delete output folder\n",
    "    os.makedirs(out)  # make new output folder\n",
    "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "    # Load model\n",
    "    attempt_download(weights)\n",
    "    model = torch.load(weights, map_location=device)['model'].float()  # load to FP32\n",
    "    # torch.save(torch.load(weights, map_location=device), weights)  # update model if SourceChangeWarning\n",
    "    # model.fuse()\n",
    "    model.to(device).eval()\n",
    "    if half:\n",
    "        model.half()  # to FP16\n",
    "\n",
    "    # Second-stage classifier\n",
    "    classify = False\n",
    "    if classify:\n",
    "        modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
    "        modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model'])  # load weights\n",
    "        modelc.to(device).eval()\n",
    "\n",
    "    # Set Dataloader\n",
    "    vid_path, vid_writer = None, None\n",
    "    if webcam:\n",
    "        view_img = True\n",
    "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "        dataset = LoadStreams(source, img_size=imgsz)\n",
    "    else:\n",
    "        save_img = True\n",
    "        dataset = LoadImages(source, img_size=imgsz)\n",
    "\n",
    "    # Get names and colors\n",
    "    names = model.names if hasattr(model, 'names') else model.modules.names\n",
    "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]\n",
    "\n",
    "    # Run inference\n",
    "    t0 = time.time()\n",
    "    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
    "    _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        pred = model(img, augment=opt.augment)[0]\n",
    "\n",
    "        # Apply NMS\n",
    "        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
    "        t2 = time_synchronized()\n",
    "\n",
    "        # Apply Classifier\n",
    "        if classify:\n",
    "            pred = apply_classifier(pred, modelc, img, im0s)\n",
    "\n",
    "        # Process detections\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "            if webcam:  # batch_size >= 1\n",
    "                p, s, im0 = path[i], '%g: ' % i, im0s[i].copy()\n",
    "            else:\n",
    "                p, s, im0 = path, '', im0s\n",
    "\n",
    "            save_path = str(Path(out) / Path(p).name)\n",
    "            s += '%gx%g ' % img.shape[2:]  # print string\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # Â normalization gain whwh\n",
    "            if det is not None and len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, -1].unique():\n",
    "                    n = (det[:, -1] == c).sum()  # detections per class\n",
    "                    s += '%g %ss, ' % (n, names[int(c)])  # add to string\n",
    "\n",
    "                # Write results\n",
    "                for *xyxy, conf, cls in det:\n",
    "                    if save_txt:  # Write to file\n",
    "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                        with open(save_path[:save_path.rfind('.')] + '.txt', 'a') as file:\n",
    "                            file.write(('%g ' * 5 + '\\n') % (cls, *xywh))  # label format\n",
    "\n",
    "                    if save_img or view_img:  # Add bbox to image\n",
    "                        label = '%s %.2f' % (names[int(cls)], conf)\n",
    "                        plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "\n",
    "            # Print time (inference + NMS)\n",
    "            print('%sDone. (%.3fs)' % (s, t2 - t1))\n",
    "\n",
    "            # Stream results\n",
    "            if view_img:\n",
    "                cv2.imshow(p, im0)\n",
    "                if cv2.waitKey(1) == ord('q'):  # q to quit\n",
    "                    raise StopIteration\n",
    "\n",
    "            # Save results (image with detections)\n",
    "            if save_img:\n",
    "                if dataset.mode == 'images':\n",
    "                    cv2.imwrite(save_path, im0)\n",
    "                else:\n",
    "                    if vid_path != save_path:  # new video\n",
    "                        vid_path = save_path\n",
    "                        if isinstance(vid_writer, cv2.VideoWriter):\n",
    "                            vid_writer.release()  # release previous video writer\n",
    "\n",
    "                        fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*opt.fourcc), fps, (w, h))\n",
    "                    vid_writer.write(im0)\n",
    "\n",
    "    if save_txt or save_img:\n",
    "        print('Results saved to %s' % os.getcwd() + os.sep + out)\n",
    "        if platform == 'darwin':  # MacOS\n",
    "            os.system('open ' + save_path)\n",
    "\n",
    "    print('Done. (%.3fs)' % (time.time() - t0))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     parser.add_argument('--weights', type=str, default='weights/yolov5s.pt', help='model.pt path')\n",
    "#     parser.add_argument('--source', type=str, default='inference/images', help='source')  # file/folder, 0 for webcam\n",
    "#     parser.add_argument('--output', type=str, default='inference/output', help='output folder')  # output folder\n",
    "#     parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n",
    "#     parser.add_argument('--conf-thres', type=float, default=0.4, help='object confidence threshold')\n",
    "#     parser.add_argument('--iou-thres', type=float, default=0.5, help='IOU threshold for NMS')\n",
    "#     parser.add_argument('--fourcc', type=str, default='mp4v', help='output video codec (verify ffmpeg support)')\n",
    "#     parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "#     parser.add_argument('--view-img', action='store_true', help='display results')\n",
    "#     parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
    "#     parser.add_argument('--classes', nargs='+', type=int, help='filter by class')\n",
    "#     parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
    "#     parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
    "#     opt = parser.parse_args()\n",
    "    \n",
    "    \n",
    "    opt.update({\n",
    "        'weights': './weights/best_wheat_detection_0.pt','source':'./test','img_size':640,\n",
    "        'output': 'inference/output','save_txt':True,'conf_thres':0.0,'iou_thres':0.6,\n",
    "        'fourcc':'mp4v','device':'','view_img':False,'device':'','classes':[0],\n",
    "        'agnostic_nms':False,'augment':False\n",
    "    })\n",
    "    \n",
    "    opt.img_size = check_img_size(opt.img_size)\n",
    "    print(opt)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        detect()\n",
    "\n",
    "        # Update all models\n",
    "        # for opt.weights in ['yolov5s.pt', 'yolov5m.pt', 'yolov5l.pt', 'yolov5x.pt', 'yolov3-spp.pt']:\n",
    "        #    detect()\n",
    "        #    create_pretrained(opt.weights, opt.weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
